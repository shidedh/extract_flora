{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import fitz\n",
    "import re\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageColor\n",
    "import operator\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ploting funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_genus_blocks(page_df, draw, color = '#6c899e', w = 3):\n",
    "    try:\n",
    "        genus_list = page_df['draw_genus'].unique()\n",
    "    except:\n",
    "        print(\"no GENUS found\")\n",
    "        return \n",
    "\n",
    "    for g in genus_list:\n",
    "        temp_df = page_df[(page_df['draw_genus'] == g)]\n",
    "        g_x0 = temp_df['x0'].min()\n",
    "        g_y0 = temp_df['y0'].min()\n",
    "        g_x1 = temp_df['x1'].max()\n",
    "        g_y1 = temp_df['y1'].max()\n",
    "\n",
    "        draw.rectangle((g_x0, g_y0, g_x1, g_y1), fill=None, outline=ImageColor.getrgb(color), width = w)\n",
    "        \n",
    "def plot_epithet_blocks(page_df, draw, color = '#660066', w = 3):\n",
    "    try:\n",
    "        epithet_list = page_df['draw_epithet'].unique()\n",
    "    except:\n",
    "        print(\"no EPITHET found\")\n",
    "        return \n",
    "    \n",
    "    for e in epithet_list:\n",
    "        temp_df = page_df[(page_df['draw_epithet'] == e)]\n",
    "        e_x0 = temp_df['x0'].min()\n",
    "        e_y0 = temp_df['y0'].min()\n",
    "        e_x1 = temp_df['x1'].max()\n",
    "        e_y1 = temp_df['y1'].max()\n",
    "\n",
    "        draw.rectangle((e_x0, e_y0, e_x1, e_y1), fill=None, outline=ImageColor.getrgb(color), width = w)\n",
    "\n",
    "def plot_author_blocks(page_df, draw, color = '#a3a3a3', w = 1):\n",
    "    try:\n",
    "        author_list = page_df['draw_author'].unique()\n",
    "    except:\n",
    "        print(\"no AUTHOR found\")\n",
    "        return \n",
    "\n",
    "    for a in author_list:\n",
    "        temp_df = page_df[(page_df['draw_author'] == a)]\n",
    "        e_x0 = temp_df['x0'].min()\n",
    "        e_y0 = temp_df['y0'].min()\n",
    "        e_x1 = temp_df['x1'].max()\n",
    "        e_y1 = temp_df['y1'].max()\n",
    "\n",
    "        draw.rectangle((e_x0, e_y0, e_x1, e_y1), fill=None, outline=ImageColor.getrgb(color), width = w)\n",
    "\n",
    "def plot_infra_blocks(page_df, draw, color = '#ff6289', w = 1):\n",
    "    try:\n",
    "        infra_list = page_df['draw_infra'].unique()\n",
    "    except:\n",
    "        print(\"no INFRA Spp. found\")\n",
    "        return \n",
    "\n",
    "    for infra_spp in infra_list:\n",
    "        temp_df = page_df[(page_df['draw_infra'] == infra_spp)]\n",
    "        e_x0 = temp_df['x0'].min()\n",
    "        e_y0 = temp_df['y0'].min()\n",
    "        e_x1 = temp_df['x1'].max()\n",
    "        e_y1 = temp_df['y1'].max()\n",
    "\n",
    "        draw.rectangle((e_x0, e_y0, e_x1, e_y1), fill=None, outline=ImageColor.getrgb(color), width = w)\n",
    "\n",
    "def plot_valid_words(page_df, draw, color = '#660044', w = 2):\n",
    "    blocks = page_df['block_no'].unique()\n",
    "    \"\"\"for b in blocks:\n",
    "        lines = page_df[page_df['block_no'] == b]['line_no'].unique()\n",
    "        for l in lines:\n",
    "            cond = (page_df['line_no'] == l) & (page_df['block_no'] == b)\n",
    "            words = page_df[cond]['word_no'].unique()\n",
    "            page_df = page_df.copy()\n",
    "            for w in words:\n",
    "                x0 = page_df[(cond) & (page_df['word_no'] == w)]['x0'].item()\n",
    "                y0 = page_df[(cond) & (page_df['word_no'] == w)]['y0'].item()\n",
    "                x1 = page_df[(cond) & (page_df['word_no'] == w)]['x1'].item()\n",
    "                y1 = page_df[(cond) & (page_df['word_no'] == w)]['y1'].item()\n",
    "                draw.rectangle((x0, y0, x1, y1), fill=None, outline=ImageColor.getrgb(color), width = w)\n",
    "    \"\"\"\n",
    "    for index, row in page_df.iterrows():\n",
    "        x0, y0, x1, y1 = row['x0'], row['y0'], row['x1'], row['y1'] \n",
    "        draw.rectangle((x0, y0, x1, y1), fill=None, outline=ImageColor.getrgb(color), width = w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Vol3 Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf_dir = \"../input/NOUVELLE FLORE DU LIBAN ET DE LA SYRIE 3.pdf\"\n",
    "pdf_dir = \"../input/NOUVELLE FLORE DU LIBAN ET DE LA SYRIE 1.pdf\"\n",
    "index = range(616, 639)\n",
    "doc = fitz.open(pdf_dir)\n",
    "pages = [doc[i] for i in range(doc.pageCount)]\n",
    "#index = list(range(555, 583))\n",
    "\n",
    "pdf_dir = \"../input/NOUVELLE FLORE DU LIBAN ET DE LA SYRIE 1.pdf\"\n",
    "index = range(616, 639)\n",
    "\n",
    "TARGET_DPI = 300\n",
    "mat = fitz.Matrix(TARGET_DPI/ 72, TARGET_DPI/ 72)\n",
    "\n",
    "indent_groups = []\n",
    "indent_err = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regex based boolean functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(word):\n",
    "    \"\"\"\n",
    "    valid words are words that are:\n",
    "    - at least 2 characters\n",
    "        - unless it's x (symbol for hybrid)\n",
    "    \"\"\"\n",
    "    return (not bool(re.search(r\"[0-9]+[,.]?\", word))) and \\\n",
    "            (word != 'NOUVELLE' and word != 'FLORE') and \\\n",
    "            (len(word) > 1 or \\\n",
    "                word == 'x' or word == 'X' or word == '×' or word == r'\\u00D7') and \\\n",
    "            ''.join(e for e in word if e.isalpha()).isalpha()\n",
    "    \n",
    "def is_genus(word):\n",
    "    \"\"\"\n",
    "    A word in the index might be a genus if it satisfies the following properties:\n",
    "    - letters: french alphabet + at most one hyphen (which is not first or last letter)\n",
    "        - first letter upper case\n",
    "        - all but first lowecase \n",
    "    in regex: ^[A-ZÀÂÄÈÉÊËÎÏÔŒÙÛÜŸÇ]{1}[a-zàâäèéêëîïôœùûüÿç]*[-]?[a-zàâäèéêëîïôœùûüÿç]+$ #ignoring strict beggining and end cause of noise\n",
    "        * based on the current expression it'd also be at least 2 letters long\n",
    "    \"\"\"\n",
    "    regex = r\"[A-ZÀÂÄÈÉÊËÎÏÔŒÙÛÜŸÇ\\u00D7]{1}[a-zàâäèéêëîïôœùûüÿç]*[-]?[a-zàâäèéêëîïôœùûüÿç]+\"\n",
    "    return re.search(regex, word)\n",
    "    \n",
    "\n",
    "def is_epithet(word):\n",
    "    \"\"\"\n",
    "    A word in the index might be an epithet if it satisfies the following properties:\n",
    "    - letters: french alphabet + at most one hyphen (which is not first or last letter)\n",
    "        - all letters lowecase \n",
    "    in regex: ^[a-zàâäèéêëîïôœùûüÿç]+[-]?[a-zàâäèéêëîïôœùûüÿç]+$ #ignoring strict beggining and end cause of noise \n",
    "        * based on the current expression it'd also be at least 2 letters long\n",
    "    \"\"\"\n",
    "    regex = r\"[a-zàâäèéêëîïôœùûüÿç\\u00D7]+[-]?[a-zàâäèéêëîïôœùûüÿç]+\"\n",
    "    return re.search(regex, word)\n",
    "    \n",
    "def is_hybrid(word):\n",
    "    regex = r\"^(([Xx\\u00D7])|([Xx\\u00D7]\\.))$\"\n",
    "    return re.search(regex, word)\n",
    "\n",
    "def is_infra(word):\n",
    "    regex = r\"^(var\\.)|(subsp\\.)\"\n",
    "    return re.search(regex, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-processing func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(page_num, indent_err = 30):\n",
    "    \n",
    "    #initiate dataframe\n",
    "    page_df = pd.DataFrame(pages[page_num].get_text_words(), columns =['in_x0', 'in_y0', 'in_x1', 'in_y1', 'word', 'block_no', 'line_no', 'word_no'])\n",
    "    \n",
    "    #add page number to dataframe\n",
    "    page_df['page_num'] = np.array([page_num]*page_df.shape[0])\n",
    "    #initiate all columns that will be added\n",
    "    page_df['genus'] = np.array([np.NaN]*page_df.shape[0])\n",
    "    page_df['draw_genus'] = np.array([np.NaN]*page_df.shape[0])\n",
    "    page_df['epithet'] = np.array([np.NaN]*page_df.shape[0])\n",
    "    page_df['draw_epithet'] = np.array([np.NaN]*page_df.shape[0])\n",
    "    page_df['author'] = np.array([np.NaN]*page_df.shape[0])\n",
    "    page_df['draw_author'] = np.array([np.NaN]*page_df.shape[0])\n",
    "    page_df['infra'] = np.array([np.NaN]*page_df.shape[0])\n",
    "    page_df['draw_infra'] = np.array([np.NaN]*page_df.shape[0])\n",
    "    page_df['taxon rank'] = np.array([np.NaN]*page_df.shape[0])\n",
    "    page_df['error_check'] = np.array([np.NaN]*page_df.shape[0])\n",
    "    #updating coordinates to represent target DPI\n",
    "    page_df['x0'], page_df['y0'], page_df['x1'], page_df['y1'] = page_df['in_x0']*TARGET_DPI/ 72, page_df['in_y0']*TARGET_DPI/ 72, page_df['in_x1']*TARGET_DPI/ 72, page_df['in_y1']*TARGET_DPI/ 72\n",
    "    #get x corner coordinates \n",
    "    x_min = page_df['x0'].min()\n",
    "    x_max = page_df['x1'].max()\n",
    "\n",
    "    y_max = page_df['y1'].max()\n",
    "\n",
    "    #Remove the extra flore - 18 at page 545\n",
    "    if page_num == index[4]:\n",
    "        page_df = page_df[~((page_df[\"word\"] == 'Flore') & (page_df['y1'] == y_max))]\n",
    "    #invalid words dataframe -- for error checking\n",
    "    pruned_words_df = page_df[~page_df[\"word\"].apply(valid)].reset_index()\n",
    "    #prune out invalid words (based on function valid)\n",
    "    page_df = page_df[page_df[\"word\"].apply(valid)].reset_index()\n",
    "    \n",
    "    indent_groups = []\n",
    "    blocks = page_df['block_no'].unique()\n",
    "    for b in blocks:\n",
    "        lines = page_df[page_df['block_no'] == b]['line_no'].unique()\n",
    "        for l in lines:\n",
    "            #reset word_no values (useful for cases where word that was originally at 0th index was pruned out)\n",
    "            cond = (page_df['line_no'] == l) & (page_df['block_no'] == b)\n",
    "            num_words = len(page_df[cond]['word_no'])\n",
    "            page_df.loc[cond, 'word_no'] = np.arange(num_words).astype(int) #this is slowww\n",
    "            #set column number (0 or 1)\n",
    "            x_0 = page_df[cond]['x0'].min()\n",
    "            #THIS DOESN'T WORK AAAA -- issue was with line no thing\n",
    "            if not np.isnan(x_0):\n",
    "                page_df.loc[cond, 'col_no'] = np.array([int(x_0 > ((x_min + x_max) / 2))]*num_words).astype(int)\n",
    "\n",
    "                #initiate indent groups -- only first word should get an indent_group value \n",
    "                new_group = True\n",
    "                for g_i in range(len(indent_groups)):\n",
    "                    g = indent_groups[g_i]\n",
    "                    g_arr = np.array(g)\n",
    "                    if x_0 <= np.mean(g_arr) + indent_err and x_0 >= np.mean(g_arr) - indent_err:\n",
    "                        g.append(x_0)\n",
    "                        new_group = False\n",
    "                        page_df.loc[cond, 'indent_group'] = np.array([g_i]*num_words).astype(int)\n",
    "                if new_group:\n",
    "                    indent_groups.append([x_0])\n",
    "                    g_i = len(indent_groups) - 1\n",
    "                    page_df.loc[cond, 'indent_group'] = np.array([g_i]*num_words).astype(int)\n",
    "\n",
    "    #print(\"indent groups:\", indent_groups)\n",
    "    #return updated page_df, pruned_words_df, indent groups\n",
    "    return page_df.reset_index(), pruned_words_df, indent_groups\n",
    "\n",
    "#https://stackoverflow.com/questions/53468558/adding-image-to-pandas-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:08<00:00,  2.69it/s]\n"
     ]
    }
   ],
   "source": [
    "genus = np.NaN\n",
    "df_dict = {}\n",
    "pruned_dict = {}\n",
    "\n",
    "for page_num in tqdm(index):\n",
    "    page_df, pruned_df, indent_group = preprocessing(page_num)\n",
    "    df_dict[page_num] = page_df\n",
    "    pruned_dict[page_num] = pruned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:06<00:00,  3.70it/s]\n"
     ]
    }
   ],
   "source": [
    "genus = np.NaN\n",
    "epithet = np.NaN\n",
    "draw_genus = np.NaN\n",
    "draw_epithet = np.NaN\n",
    "result_ims = []\n",
    "df_list = []\n",
    "\n",
    "for page_num in tqdm(index):\n",
    "    #page_num = index[-1]\n",
    "    #process the pre-processed dfs\n",
    "    page_df = df_dict[page_num]\n",
    "    \n",
    "    #for drawing\n",
    "    pix_map = doc.get_page_pixmap(page_num,matrix=mat)\n",
    "    image = Image.open(io.BytesIO(pix_map.tobytes()))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    plot_valid_words(page_df, draw, color = '#660044', w = 2)\n",
    "    result_ims.append(image)\n",
    "    \n",
    "    #break \n",
    "TIME_STR = datetime.now().strftime(\"%Y_%m_%d-%I_%M_%p\")\n",
    "result_ims[0].save('../output/index/PDF/vol1_index_valid_words'+TIME_STR+'.pdf',save_all=True, append_images=result_ims[1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding indentations associated with genus, epithet, infra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['genus', 'epithet', 'infra', 'author', 'misc.']\n",
    "def n_leftmost_indent(df, n):\n",
    "    \"\"\"return a tuple with at most 3 elements each element itself is a tuple containing indent group, mean, group len\"\"\"\n",
    "    indent_groups = [(g, df[(df['indent_group'] == g) & (df['word_no'] == 0)]['x0'].mean(), len(df[(df['indent_group'] == g) & (df['word_no'] == 0)]['x0'])) for g in df['indent_group'].unique()]\n",
    "    indent_groups.sort(key = lambda x : x[1])\n",
    "    print(indent_groups[:n])\n",
    "    return indent_groups[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genusEpithetInfra_indent(col_df):\n",
    "    leftmost_3_indents = n_leftmost_indent(col_df, 2) \n",
    "    min_gap = 0\n",
    "    max_gap = 75\n",
    "\n",
    "    # possibly not specific enough\n",
    "    # first identifying indent based don distance from one another only\n",
    "    \"\"\"if len(leftmost_3_indents) == 3:\n",
    "        if leftmost_3_indents[0][1] < max_gap:\n",
    "            leftmost_3_indents = leftmost_3_indents[1:]\n",
    "        elif ((leftmost_3_indents[1][1] - leftmost_3_indents[0][1]) > max_gap or \\\n",
    "            (leftmost_3_indents[1][1] - leftmost_3_indents[0][1]) < min_gap): #comparing first two (if satisfied last two will be checked in next if block)\n",
    "            leftmost_3_indents = [max(leftmost_3_indents[1:], key = lambda x : x[2])] + [leftmost_3_indents[2]]\n",
    "        elif (leftmost_3_indents[2][1] - leftmost_3_indents[1][1]) > max_gap or \\\n",
    "            (leftmost_3_indents[2][1] - leftmost_3_indents[1][1]) < min_gap: #comparing last two\n",
    "            leftmost_3_indents = [leftmost_3_indents[0]] + [max(leftmost_3_indents[1:], key = lambda x : x[2])]\n",
    "\n",
    "    if len(leftmost_3_indents) == 2:\n",
    "        if leftmost_3_indents[0][1] < max_gap:\n",
    "            leftmost_3_indents = leftmost_3_indents[1]\n",
    "        elif (leftmost_3_indents[1][1] - leftmost_3_indents[0][1]) > max_gap or (leftmost_3_indents[1][1] - leftmost_3_indents[0][1]) < min_gap:\n",
    "            leftmost_3_indents = [max(leftmost_3_indents, key = lambda x : x[2])]\"\"\"\n",
    "\n",
    "    has_genus, has_epithet, has_infra = False, False, False\n",
    "    genus_indent, epithet_indent, infra_indent = -1, -1, -1\n",
    "    if len(leftmost_3_indents) == 3 and type(leftmost_3_indents) == type([1,2,3]):\n",
    "        has_genus, has_epithet, has_infra = True, True, True\n",
    "        print(\"leftmost 3:\", leftmost_3_indents)\n",
    "        genus_indent, epithet_indent, infra_indent = [el[0] for el in leftmost_3_indents]\n",
    "    elif len(leftmost_3_indents) == 2:\n",
    "        if col_df[col_df['indent_group'] == leftmost_3_indents[1][0]]['word'].apply(is_infra).any():\n",
    "            has_genus, has_epithet, has_infra = False, True, True\n",
    "            epithet_indent, infra_indent = [el[0] for el in leftmost_3_indents]\n",
    "        else:\n",
    "            has_genus, has_epithet, has_infra = True, True, False\n",
    "            genus_indent, epithet_indent = [el[0] for el in leftmost_3_indents]\n",
    "    elif len(leftmost_3_indents) == 1 or type(leftmost_3_indents) == type((1,2,3)): \n",
    "        if type(leftmost_3_indents) == type((1,2,3)):\n",
    "            leftmost_3_indents = [leftmost_3_indents]\n",
    "        has_genus, has_epithet, has_infra = False, True, False\n",
    "        epithet_indent = leftmost_3_indents[0][0]\n",
    "\n",
    "    return genus_indent, epithet_indent, infra_indent, leftmost_3_indents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing column dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_col(col_df, genus, epithet, draw_genus, draw_epithet, draw_infra = np.NaN):\n",
    "    genus_indent, epithet_indent, infra_indent, indent_3_left = get_genusEpithetInfra_indent(col_df)\n",
    "    print(genus_indent, epithet_indent, infra_indent, indent_3_left)\n",
    "    \n",
    "    blocks = col_df['block_no'].unique()\n",
    "    start_word_cond = -1 \n",
    "    author = ''\n",
    "    #draw_infra = np.NaN\n",
    "    \n",
    "    for b in blocks:\n",
    "        lines = col_df[col_df['block_no'] == b]['line_no'].unique()\n",
    "        for l in lines:\n",
    "            cond = (col_df['line_no'] == l) & (col_df['block_no'] == b)\n",
    "            words = col_df[cond]['word_no'].unique()\n",
    "            process_hybrid = False\n",
    "            process_infra = False\n",
    "            \n",
    "            col_df = col_df.copy()\n",
    "            for w in words:\n",
    "                word_cond = (col_df['line_no'] == l) & (col_df['block_no'] == b) & (col_df['word_no'] == w) \n",
    "                word = col_df[word_cond]['word'].item()\n",
    "                #print(word)\n",
    "\n",
    "                if w == 0:\n",
    "                    infra = ''\n",
    "                    if author != '':\n",
    "                        col_df.loc[start_word_cond, 'author'] = author\n",
    "                        author = ''\n",
    "                    \n",
    "                    start_word_cond = word_cond\n",
    "                    start_l = l \n",
    "                    start_b = b \n",
    "\n",
    "                    indent_group = col_df[word_cond]['indent_group'].item()\n",
    "                    \n",
    "                    if is_hybrid(word):\n",
    "                        process_hybrid = True\n",
    "                        misc = word\n",
    "                        author = ''\n",
    "                        #col_df.loc[start_word_cond, 'misc.'] = misc\n",
    "                    #now only gotta say INDENT AND satisfies these paterns\n",
    "                    #print(indent_group, genus_indent)\n",
    "                    else: \n",
    "                        if indent_group == genus_indent:\n",
    "                            if not ''.join(e for e in word if e.isalpha()).isupper():\n",
    "                                genus = word\n",
    "                                misc = ''\n",
    "                                author = ''\n",
    "                                infra = ''\n",
    "                                epithet = ''\n",
    "                                draw_genus = genus\n",
    "                                col_df.loc[start_word_cond, 'genus'] = genus\n",
    "                                col_df.loc[start_word_cond, 'taxon rank'] = 'genus'\n",
    "                                if not is_genus(word):\n",
    "                                    col_df.loc[start_word_cond, 'error_check'] = True\n",
    "                                col_df.loc[word_cond, 'draw_genus'] = draw_genus\n",
    "                                \n",
    "                            else: \n",
    "                                genus = ''\n",
    "                                misc = ''\n",
    "                                author = ''\n",
    "                                infra = ''\n",
    "                                epithet = ''\n",
    "                                draw_genus = np.NaN\n",
    "                            \"\"\"\n",
    "                            def set_genus(df, w0_index, error_check = True):\n",
    "                                if not ''.join(e for e in word if e.isalpha()).isupper():\n",
    "                                    genus = word\n",
    "                                    draw_genus = genus\n",
    "                                    df.loc[w0_index, 'genus'] = genus\n",
    "                                    df.loc[w0_index, 'taxon rank'] = 'genus'\n",
    "                                    col_df.loc[word_cond, 'draw_genus'] = draw_genus\"\"\"\n",
    "                        elif indent_group == epithet_indent and not ''.join(e for e in word if e.isalpha()).isupper():\n",
    "                            epithet = word\n",
    "                            misc = ''\n",
    "                            infra = ''\n",
    "                            author = ''\n",
    "                            col_df.loc[start_word_cond, 'genus'] = genus\n",
    "                            col_df.loc[start_word_cond, 'epithet'] = epithet\n",
    "                            col_df.loc[start_word_cond, 'taxon rank'] = 'species'\n",
    "                            if not is_epithet(word):\n",
    "                                col_df.loc[start_word_cond, 'error_check'] = True\n",
    "                            draw_epithet = str(genus) + '_' + str(epithet) +'_' + str(start_b) + '_' + str(start_l)\n",
    "                            col_df.loc[word_cond, 'draw_genus'] = draw_genus\n",
    "                            col_df.loc[word_cond, 'draw_epithet'] = draw_epithet\n",
    "                        \n",
    "\n",
    "                        elif indent_group == infra_indent:\n",
    "                            process_infra = True\n",
    "                            misc = word\n",
    "                            author = ''\n",
    "                            #col_df.loc[start_word_cond, 'misc.'] = misc\n",
    "                            if not (is_infra(word) or is_hybrid(word)):\n",
    "                                col_df.loc[start_word_cond, 'error_check'] = True\n",
    "                    \n",
    "                elif process_infra:\n",
    "                    start_word_cond = word_cond\n",
    "                    start_l = l \n",
    "                    start_b = b \n",
    "                    infra = word \n",
    "                    col_df.loc[start_word_cond, 'genus'] = genus\n",
    "                    col_df.loc[start_word_cond, 'epithet'] = epithet\n",
    "                    col_df.loc[start_word_cond, 'infra'] = infra\n",
    "                    #col_df.loc[start_word_cond, 'misc.'] = misc\n",
    "                    col_df.loc[start_word_cond, 'taxon rank'] = misc\n",
    "                    draw_infra = str(infra) + '_'+str(start_b)+'_'+str(start_l)\n",
    "                    process_infra = False\n",
    "                    col_df.loc[word_cond, 'draw_genus'] = draw_genus\n",
    "                    col_df.loc[word_cond, 'draw_epithet'] = draw_epithet\n",
    "                    col_df.loc[word_cond, 'draw_infra'] = draw_infra\n",
    "                    \n",
    "                elif process_hybrid:\n",
    "                    start_word_cond = word_cond\n",
    "                    start_l = l \n",
    "                    start_b = b \n",
    "                    if indent_group == genus_indent:\n",
    "                        genus = word\n",
    "                        epithet = ''\n",
    "                        infra = ''\n",
    "                        author = ''\n",
    "                        draw_genus = genus\n",
    "                        col_df.loc[start_word_cond, 'genus'] = genus\n",
    "                        col_df.loc[start_word_cond, 'taxon rank'] = 'genus - hybrid'\n",
    "                        if not is_genus(word):\n",
    "                            col_df.loc[start_word_cond, 'error_check'] = True\n",
    "                        col_df.loc[word_cond, 'draw_genus'] = draw_genus\n",
    "                            \n",
    "                    elif indent_group == epithet_indent:\n",
    "                        epithet = word\n",
    "                        author = ''\n",
    "                        infra = ''\n",
    "                        col_df.loc[start_word_cond, 'genus'] = genus\n",
    "                        col_df.loc[start_word_cond, 'epithet'] = epithet\n",
    "                        col_df.loc[start_word_cond, 'taxon rank'] = 'species - hybrid'\n",
    "                        draw_epithet = str(genus) + '_' + str(epithet) +'_' + str(start_b) + '_' + str(start_l)\n",
    "                        if not is_epithet(word):\n",
    "                            col_df.loc[start_word_cond, 'error_check'] = True\n",
    "                        col_df.loc[word_cond, 'draw_genus'] = draw_genus\n",
    "                        col_df.loc[word_cond, 'draw_epithet'] = draw_epithet\n",
    "                    elif indent_group == infra_indent:\n",
    "                        infra = word\n",
    "                        col_df.loc[start_word_cond, 'genus'] = genus\n",
    "                        col_df.loc[start_word_cond, 'epithet'] = epithet\n",
    "                        col_df.loc[start_word_cond, 'infra'] = infra\n",
    "                        col_df.loc[start_word_cond, 'taxon rank'] = 'hybrid'\n",
    "                        col_df.loc[word_cond, 'draw_genus'] = draw_genus\n",
    "                        col_df.loc[word_cond, 'draw_epithet'] = draw_epithet\n",
    "                        col_df.loc[word_cond, 'draw_infra'] = draw_infra\n",
    "                    #col_df.loc[start_word_cond, 'misc.'] = 'x'\n",
    "                    process_hybrid = False\n",
    "                else:\n",
    "                    if w == 1 and epithet == '': \n",
    "                        epithet = word\n",
    "                        misc = ''\n",
    "                        infra = ''\n",
    "                        author = ''\n",
    "                        col_df.loc[start_word_cond, 'genus'] = genus\n",
    "                        col_df.loc[start_word_cond, 'epithet'] = epithet\n",
    "                        col_df.loc[start_word_cond, 'taxon rank'] = 'species'\n",
    "                        if not is_epithet(word):\n",
    "                            col_df.loc[start_word_cond, 'error_check'] = True\n",
    "                        draw_epithet = str(genus) + '_' + str(epithet) +'_' + str(start_b) + '_' + str(start_l)\n",
    "                        col_df.loc[word_cond, 'draw_genus'] = draw_genus\n",
    "                        col_df.loc[word_cond, 'draw_epithet'] = draw_epithet\n",
    "                    #elif word == 'FLORE':\n",
    "                    #    author = ''\n",
    "                    else:\n",
    "                        author = author + word + ' '\n",
    "                        col_df.loc[word_cond, 'draw_author'] = 'author_'+str(start_b)+'_'+str(start_l)\n",
    "                    #col_df.loc[word_cond, 'draw_genus'] = draw_genus\n",
    "                    #if epithet:\n",
    "                    #    col_df.loc[word_cond, 'draw_epithet'] = draw_epithet\n",
    "                    #if infra: \n",
    "                    #    col_df.loc[word_cond, 'draw_infra'] = draw_infra\n",
    "\n",
    "    #Last author\n",
    "    if author != '':\n",
    "        col_df.loc[start_word_cond, 'author'] = author\n",
    "    \n",
    "    return col_df, genus, epithet, draw_genus, draw_epithet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_col(col_df, genus, epithet, draw_genus, draw_epithet, draw_infra = np.NaN):\n",
    "    genus_indent, epithet_indent, infra_indent, indent_3_left = get_genusEpithetInfra_indent(col_df)\n",
    "    print(genus_indent, epithet_indent, infra_indent, indent_3_left)\n",
    "    \n",
    "    blocks = col_df['block_no'].unique()\n",
    "    start_word_cond = -1 \n",
    "    author = ''\n",
    "    #draw_infra = np.NaN\n",
    "    \n",
    "    col_df = col_df.copy()\n",
    "    for index, row in col_df.iterrows():\n",
    "        b, l, w = row['block_no'], row['line_no'], row['word_no']\n",
    "        word, indent_group = row['word'], row['indent_group']\n",
    "        row_cond = (col_df['line_no'] == l) & (col_df['block_no'] == b) & (col_df['word_no'] == w) \n",
    "        process_hybrid = False\n",
    "        process_infra = False\n",
    "        if w == 0: \n",
    "            start_word_cond = row_cond\n",
    "            if indent_group == genus_indent and not ''.join(e for e in word if e.isalpha()).isupper():\n",
    "                genus = word\n",
    "                draw_genus = genus\n",
    "                epithet = ''\n",
    "                draw_epithet = ''\n",
    "                author = ''\n",
    "                misc = ''\n",
    "                infra = ''\n",
    "                col_df.loc[start_word_cond, 'genus'] = genus\n",
    "                col_df.loc[start_word_cond, 'taxon rank'] = 'genus'\n",
    "                if not is_genus(word):\n",
    "                    col_df.loc[row_cond, 'error_check'] = True\n",
    "                col_df.loc[row_cond, 'draw_genus'] = draw_genus\n",
    "                col_df.loc[row_cond, 'author'] = ''\n",
    "\n",
    "            elif indent_group == epithet_indent and not ''.join(e for e in word if e.isalpha()).isupper():\n",
    "                epithet = word\n",
    "                author = ''\n",
    "                col_df.loc[row_cond, 'genus'] = genus\n",
    "                col_df.loc[row_cond, 'epithet'] = epithet\n",
    "                col_df.loc[row_cond, 'taxon rank'] = 'species'\n",
    "                if not is_epithet(word):\n",
    "                    col_df.loc[row_cond, 'error_check'] = True\n",
    "                draw_epithet = str(genus) + '_' + str(epithet) +'_' + str(b) + '_' + str(l)\n",
    "                col_df.loc[row_cond, 'draw_genus'] = draw_genus\n",
    "                col_df.loc[row_cond, 'draw_epithet'] = draw_epithet\n",
    "                col_df.loc[row_cond, 'author'] = ''\n",
    "        \n",
    "        else:\n",
    "            if w == 1 and epithet == '': \n",
    "                epithet = word\n",
    "                misc = ''\n",
    "                infra = ''\n",
    "                author = ''\n",
    "                start_word_cond = row_cond\n",
    "                col_df.loc[row_cond, 'genus'] = genus\n",
    "                col_df.loc[row_cond, 'epithet'] = epithet\n",
    "                col_df.loc[row_cond, 'taxon rank'] = 'species'\n",
    "                if not is_epithet(word):\n",
    "                    col_df.loc[row_cond, 'error_check'] = True\n",
    "                draw_epithet = str(genus) + '_' + str(epithet) +'_' + str(b) + '_' + str(l)\n",
    "                col_df.loc[row_cond, 'draw_genus'] = draw_genus\n",
    "                col_df.loc[row_cond, 'draw_epithet'] = draw_epithet\n",
    "                col_df.loc[row_cond, 'author'] = ''\n",
    "            #elif word == 'FLORE':\n",
    "            #    author = ''\n",
    "            else:\n",
    "                #print(col_df.loc[start_word_cond, 'author'])\n",
    "                \"\"\"if np.isnan(col_df.loc[start_word_cond, 'author'].item()):\n",
    "                    author == ''\n",
    "                    col_df.loc[start_word_cond, 'author'] = ''\"\"\"\n",
    "                curr_author_part = word + ' '\n",
    "                col_df.loc[start_word_cond, 'author'] += curr_author_part\n",
    "                col_df.loc[row_cond, 'draw_author'] = 'author_'+str(b)+'_'+str(l)\n",
    "            #col_df.loc[word_cond, 'draw_genus'] = draw_genus\n",
    "            #if epithet:\n",
    "            #    col_df.loc[word_cond, 'draw_epithet'] = draw_epithet\n",
    "            #if infra: \n",
    "            #    col_df.loc[word_cond, 'draw_infra'] = draw_infra\"\"\"\n",
    "\n",
    "    #Last author\n",
    "    \"\"\"if author != '':\n",
    "        col_df.loc[start_word_cond, 'author'] = author\"\"\"\n",
    "                    \n",
    "\n",
    "    return col_df, genus, epithet, draw_genus, draw_epithet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2.0, 116.37499829133353, 8), (1.0, 172.15384744171408, 39)]\n",
      "2.0 1.0 -1 [(2.0, 116.37499829133353, 8), (1.0, 172.15384744171408, 39)]\n",
      "[(3.0, 972.6666627106845, 9), (4.0, 1030.1499970753987, 40)]\n",
      "3.0 4.0 -1 [(3.0, 972.6666627106845, 9), (4.0, 1030.1499970753987, 40)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/23 [00:01<00:29,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 123.00000190734863, 2), (0.0, 184.4999977520534, 56)]\n",
      "1.0 0.0 -1 [(1.0, 123.00000190734863, 2), (0.0, 184.4999977520534, 56)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 2/23 [00:02<00:25,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6.0, 985.428573971703, 7), (5.0, 1039.823529611226, 51)]\n",
      "6.0 5.0 -1 [(6.0, 985.428573971703, 7), (5.0, 1039.823529611226, 51)]\n",
      "[(0.0, 81.60000085830688, 10), (1.0, 137.95833239952722, 48)]\n",
      "0.0 1.0 -1 [(0.0, 81.60000085830688, 10), (1.0, 137.95833239952722, 48)]\n",
      "[(4.0, 938.3333365122479, 12), (3.0, 994.3043418552568, 46)]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 3/23 [00:03<00:21,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4.0 3.0 -1 [(4.0, 938.3333365122479, 12), (3.0, 994.3043418552568, 46)]\n",
      "[(1.0, 234.29999987284344, 10), (0.0, 289.5625001854367, 48)]\n",
      "1.0 0.0 -1 [(1.0, 234.29999987284344, 10), (0.0, 289.5625001854367, 48)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 4/23 [00:04<00:19,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2.0, 1090.0999704996743, 10), (3.0, 1146.2083260218303, 48)]\n",
      "2.0 3.0 -1 [(2.0, 1090.0999704996743, 10), (3.0, 1146.2083260218303, 48)]\n",
      "[(1.0, 89.00000037568988, 11), (0.0, 145.8409092643044, 44)]\n",
      "1.0 0.0 -1 [(1.0, 89.00000037568988, 11), (0.0, 145.8409092643044, 44)]\n",
      "[(3.0, 943.7692397680039, 13), (2.0, 998.5777720698605, 45)]\n",
      "3.0 2.0 -1 [(3.0, 943.7692397680039, 13), (2.0, 998.5777720698605, 45)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 5/23 [00:05<00:18,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 185.77777897870098, 9), (0.0, 244.34042518020522, 47)]\n",
      "1.0 0.0 -1 [(1.0, 185.77777897870098, 9), (0.0, 244.34042518020522, 47)]\n",
      "[(3.0, 1040.6666808658176, 12), (2.0, 1100.152173249618, 46)]\n",
      "3.0 2.0 -1 [(3.0, 1040.6666808658176, 12), (2.0, 1100.152173249618, 46)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 6/23 [00:06<00:16,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 136.70000076293942, 10), (0.0, 192.31250087420145, 48)]\n",
      "1.0 0.0 -1 [(1.0, 136.70000076293942, 10), (0.0, 192.31250087420145, 48)]\n",
      "[(3.0, 991.2857101077124, 14), (2.0, 1046.6136397737446, 44)]\n",
      "3.0 2.0 -1 [(3.0, 991.2857101077124, 14), (2.0, 1046.6136397737446, 44)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 7/23 [00:07<00:15,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 217.60000228881836, 15), (0.0, 274.4051164434861, 43)]\n",
      "1.0 0.0 -1 [(1.0, 217.60000228881836, 15), (0.0, 274.4051164434861, 43)]\n",
      "[(2.0, 1073.8888775860823, 9), (3.0, 1129.9326037684232, 47)]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 8/23 [00:08<00:14,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.0 3.0 -1 [(2.0, 1073.8888775860823, 9), (3.0, 1129.9326037684232, 47)]\n",
      "[(1.0, 102.99999872843425, 15), (0.0, 158.6585358875554, 41)]\n",
      "1.0 0.0 -1 [(1.0, 102.99999872843425, 15), (0.0, 158.6585358875554, 41)]\n",
      "[(3.0, 961.5555515995732, 18), (2.0, 1016.8947336966529, 38)]\n",
      "3.0 2.0 -1 [(3.0, 961.5555515995732, 18), (2.0, 1016.8947336966529, 38)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 9/23 [00:09<00:14,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 130.12500007947287, 8), (0.0, 183.08333158493042, 48)]\n",
      "1.0 0.0 -1 [(1.0, 130.12500007947287, 8), (0.0, 183.08333158493042, 48)]\n",
      "[(2.0, 988.1249984105427, 16), (3.0, 1041.9385985722615, 42)]\n",
      "2.0 3.0 -1 [(2.0, 988.1249984105427, 16), (3.0, 1041.9385985722615, 42)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 10/23 [00:10<00:13,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 97.66666889190674, 15), (0.0, 153.14634214571822, 41)]\n",
      "1.0 0.0 -1 [(1.0, 97.66666889190674, 15), (0.0, 153.14634214571822, 41)]\n",
      "[(2.0, 956.1999893188477, 15), (3.0, 1012.2517696646755, 43)]\n",
      "2.0 3.0 -1 [(2.0, 956.1999893188477, 15), (3.0, 1012.2517696646755, 43)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 11/23 [00:11<00:12,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 144.00000042385525, 6), (0.0, 195.4062379201253, 50)]\n",
      "1.0 0.0 -1 [(1.0, 144.00000042385525, 6), (0.0, 195.4062379201253, 50)]\n",
      "[(4.0, 1000.0769150562775, 13), (3.0, 1057.0412667592368, 40)]\n",
      "4.0 3.0 -1 [(4.0, 1000.0769150562775, 13), (3.0, 1057.0412667592368, 40)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 12/23 [00:12<00:11,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 105.38095216902474, 21), (0.0, 161.05882373510622, 34)]\n",
      "1.0 0.0 -1 [(1.0, 105.38095216902474, 21), (0.0, 161.05882373510622, 34)]\n",
      "[(3.0, 967.5333404541016, 15), (4.0, 1022.4102558233793, 39)]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 13/23 [00:13<00:09,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3.0 4.0 -1 [(3.0, 967.5333404541016, 15), (4.0, 1022.4102558233793, 39)]\n",
      "[(1.0, 118.6363624803948, 11), (0.0, 175.39534827535468, 43)]\n",
      "1.0 0.0 -1 [(1.0, 118.6363624803948, 11), (0.0, 175.39534827535468, 43)]\n",
      "[(6.0, 980.9285708836145, 14), (5.0, 1036.6097582064992, 41)]\n",
      "6.0 5.0 -1 [(6.0, 980.9285708836145, 14), (5.0, 1036.6097582064992, 41)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 14/23 [00:15<00:12,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 79.49999968210855, 4), (0.0, 134.2222238764351, 54)]\n",
      "1.0 0.0 -1 [(1.0, 79.49999968210855, 4), (0.0, 134.2222238764351, 54)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 15/23 [00:16<00:10,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4.0, 941.0714285714286, 14), (3.0, 996.7857103499155, 42)]\n",
      "4.0 3.0 -1 [(4.0, 941.0714285714286, 14), (3.0, 996.7857103499155, 42)]\n",
      "[(0.0, 139.76922768812912, 13), (1.0, 195.79545548467922, 44)]\n",
      "0.0 1.0 -1 [(0.0, 139.76922768812912, 13), (1.0, 195.79545548467922, 44)]\n",
      "[(5.0, 1000.9999956403461, 14), (4.0, 1056.6136259021182, 44)]\n",
      "5.0 4.0 -1 [(5.0, 1000.9999956403461, 14), (4.0, 1056.6136259021182, 44)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 16/23 [00:18<00:09,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 80.88889122009277, 9), (0.0, 136.11077710884769, 49)]\n",
      "1.0 0.0 -1 [(1.0, 80.88889122009277, 9), (0.0, 136.11077710884769, 49)]\n",
      "[(4.0, 939.8750066757202, 16), (3.0, 995.4761823018392, 42)]\n",
      "4.0 3.0 -1 [(4.0, 939.8750066757202, 16), (3.0, 995.4761823018392, 42)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 17/23 [00:19<00:08,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 147.55555435463233, 9), (0.0, 204.48837132416952, 43)]\n",
      "1.0 0.0 -1 [(1.0, 147.55555435463233, 9), (0.0, 204.48837132416952, 43)]\n",
      "[(3.0, 1011.3333384195963, 6), (4.0, 1067.4799982706706, 50)]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 18/23 [00:20<00:05,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3.0 4.0 -1 [(3.0, 1011.3333384195963, 6), (4.0, 1067.4799982706706, 50)]\n",
      "[(0.0, 81.14285809653146, 7), (1.0, 132.1176474390466, 51)]\n",
      "0.0 1.0 -1 [(0.0, 81.14285809653146, 7), (1.0, 132.1176474390466, 51)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 19/23 [00:21<00:04,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3.0, 940.0833394792345, 12), (2.0, 993.0434807487156, 46)]\n",
      "3.0 2.0 -1 [(3.0, 940.0833394792345, 12), (2.0, 993.0434807487156, 46)]\n",
      "[(1.0, 131.00000023841858, 12), (0.0, 187.06521849701372, 46)]\n",
      "1.0 0.0 -1 [(1.0, 131.00000023841858, 12), (0.0, 187.06521849701372, 46)]\n",
      "[(2.0, 1047.7413736540695, 58)]\n",
      "-1 2.0 -1 [(2.0, 1047.7413736540695, 58)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 20/23 [00:22<00:03,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 80.437500278155, 16), (0.0, 136.95238090696787, 42)]\n",
      "1.0 0.0 -1 [(1.0, 80.437500278155, 16), (0.0, 136.95238090696787, 42)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 21/23 [00:22<00:01,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4.0, 940.5833350287545, 12), (3.0, 997.4090922962537, 44)]\n",
      "4.0 3.0 -1 [(4.0, 940.5833350287545, 12), (3.0, 997.4090922962537, 44)]\n",
      "[(0.0, 169.77777657685454, 9), (1.0, 222.23404248555497, 47)]\n",
      "0.0 1.0 -1 [(0.0, 169.77777657685454, 9), (1.0, 222.23404248555497, 47)]\n",
      "[(4.0, 1029.3999989827475, 15), (3.0, 1083.5791093331798, 36)]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 22/23 [00:23<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4.0 3.0 -1 [(4.0, 1029.3999989827475, 15), (3.0, 1083.5791093331798, 36)]\n",
      "[(0.0, 59.99999841054281, 1), (1.0, 115.91666738192242, 12)]\n",
      "0.0 1.0 -1 [(0.0, 59.99999841054281, 1), (1.0, 115.91666738192242, 12)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:24<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2.0, 918.374999364217, 8), (3.0, 973.9999771118164, 2)]\n",
      "2.0 3.0 -1 [(2.0, 918.374999364217, 8), (3.0, 973.9999771118164, 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "genus = np.NaN\n",
    "epithet = np.NaN\n",
    "draw_genus = np.NaN\n",
    "draw_epithet = np.NaN\n",
    "result_ims = []\n",
    "df_list = []\n",
    "\n",
    "for page_num in tqdm(index):\n",
    "    #if page_num == index[-2]:\n",
    "    #    break\n",
    "    #page_num = index[-1]\n",
    "    #process the pre-processed dfs\n",
    "    page_df = df_dict[page_num]\n",
    "    \n",
    "    #for drawing\n",
    "    pix_map = doc.get_page_pixmap(page_num,matrix=mat)\n",
    "    image = Image.open(io.BytesIO(pix_map.tobytes()))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    #processing each column\n",
    "    for c in page_df['col_no'].unique():\n",
    "        col_df = page_df[page_df['col_no'] == c]\n",
    "        col_df, genus, epithet, draw_genus, draw_epithet = process_col(col_df, genus, epithet, draw_genus, draw_epithet)\n",
    "        df_list.append(col_df)\n",
    "\n",
    "        #drawing boxes in each column\n",
    "        plot_genus_blocks(col_df, draw)\n",
    "        plot_epithet_blocks(col_df, draw)\n",
    "        plot_author_blocks(col_df, draw)\n",
    "        plot_infra_blocks(col_df, draw)\n",
    "\n",
    "    result_ims.append(image)\n",
    "    #break \n",
    "TIME_STR = datetime.now().strftime(\"%Y_%m_%d-%I_%M_%p\")\n",
    "result_ims[0].save('../output/index/PDF/vol1_index_reverted_ROI'+TIME_STR+'.pdf',save_all=True, append_images=result_ims[1:])\n",
    "\n",
    "pre_processed_df = pd.concat([df_dict[k] for k in df_dict], axis = 0)\n",
    "df = pd.concat(df_list, axis = 0)\n",
    "df.to_html('../output/index/html/vol1_index_reverted'+TIME_STR+'.html')\n",
    "pre_processed_df.to_html('../output/index/html/vol1_preprocessed_index'+TIME_STR+'.html')\n",
    "df.to_csv('../output/index/CSV/vol1_index_reverted'+TIME_STR+'.csv', index = False)\n",
    "\n",
    "pruned = df[(~df['genus'].isnull())]\n",
    "pruned = pruned[[\"page_num\", \"genus\", \"epithet\", \"infra\" ,\"author\", \"taxon rank\"]]\n",
    "pruned.to_csv('../output/index/CSV/vol1_index_reverted_pruned'+TIME_STR+'.csv', index = False)\n",
    "pruned.to_html('../output/index/html/vol1_index_reverted_pruned'+TIME_STR+'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_list[-1]\n",
    "#df_dict[index[-1]]\n",
    "#result_ims[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result_ims[-1]\n",
    "\"Nées\".isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}