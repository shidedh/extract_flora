{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import fitz\n",
    "import re\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageColor\n",
    "import operator\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ploting funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_genus_blocks(page_df, draw, color = '#6c899e', w = 3):\n",
    "    try:\n",
    "        genus_list = page_df['draw_genus'].unique()\n",
    "    except:\n",
    "        #print(\"no GENUS found\")\n",
    "        return \n",
    "\n",
    "    for g in genus_list:\n",
    "        temp_df = page_df[(page_df['draw_genus'] == g)]\n",
    "        g_x0 = temp_df['x0'].min()\n",
    "        g_y0 = temp_df['y0'].min()\n",
    "        g_x1 = temp_df['x1'].max()\n",
    "        g_y1 = temp_df['y1'].max()\n",
    "\n",
    "        draw.rectangle((g_x0, g_y0, g_x1, g_y1), fill=None, outline=ImageColor.getrgb(color), width = w)\n",
    "        \n",
    "def plot_epithet_blocks(page_df, draw, color = '#660066', w = 3):\n",
    "    try:\n",
    "        epithet_list = page_df['draw_epithet'].unique()\n",
    "    except:\n",
    "        print(\"no EPITHET found\")\n",
    "        return \n",
    "    \n",
    "    for e in epithet_list:\n",
    "        temp_df = page_df[(page_df['draw_epithet'] == e)]\n",
    "        e_x0 = temp_df['x0'].min()\n",
    "        e_y0 = temp_df['y0'].min()\n",
    "        e_x1 = temp_df['x1'].max()\n",
    "        e_y1 = temp_df['y1'].max()\n",
    "\n",
    "        draw.rectangle((e_x0, e_y0, e_x1, e_y1), fill=None, outline=ImageColor.getrgb(color), width = w)\n",
    "\n",
    "def plot_author_blocks(page_df, draw, color = '#a3a3a3', w = 1):\n",
    "    try:\n",
    "        author_list = page_df['draw_author'].unique()\n",
    "    except:\n",
    "        print(\"no AUTHOR found\")\n",
    "        return \n",
    "\n",
    "    for a in author_list:\n",
    "        temp_df = page_df[(page_df['draw_author'] == a)]\n",
    "        e_x0 = temp_df['x0'].min()\n",
    "        e_y0 = temp_df['y0'].min()\n",
    "        e_x1 = temp_df['x1'].max()\n",
    "        e_y1 = temp_df['y1'].max()\n",
    "\n",
    "        draw.rectangle((e_x0, e_y0, e_x1, e_y1), fill=None, outline=ImageColor.getrgb(color), width = w)\n",
    "\n",
    "def plot_infra_blocks(page_df, draw, color = '#ff6289', w = 1):\n",
    "    try:\n",
    "        infra_list = page_df['draw_infra'].unique()\n",
    "    except:\n",
    "        print(\"no INFRA Spp. found\")\n",
    "        return \n",
    "\n",
    "    for infra_spp in infra_list:\n",
    "        temp_df = page_df[(page_df['draw_infra'] == infra_spp)]\n",
    "        e_x0 = temp_df['x0'].min()\n",
    "        e_y0 = temp_df['y0'].min()\n",
    "        e_x1 = temp_df['x1'].max()\n",
    "        e_y1 = temp_df['y1'].max()\n",
    "\n",
    "        draw.rectangle((e_x0, e_y0, e_x1, e_y1), fill=None, outline=ImageColor.getrgb(color), width = w)\n",
    "\n",
    "def plot_valid_words(page_df, draw, color = '#660044', w = 2):\n",
    "    blocks = page_df['block_no'].unique()\n",
    "    \"\"\"for b in blocks:\n",
    "        lines = page_df[page_df['block_no'] == b]['line_no'].unique()\n",
    "        for l in lines:\n",
    "            cond = (page_df['line_no'] == l) & (page_df['block_no'] == b)\n",
    "            words = page_df[cond]['word_no'].unique()\n",
    "            page_df = page_df.copy()\n",
    "            for w in words:\n",
    "                x0 = page_df[(cond) & (page_df['word_no'] == w)]['x0'].item()\n",
    "                y0 = page_df[(cond) & (page_df['word_no'] == w)]['y0'].item()\n",
    "                x1 = page_df[(cond) & (page_df['word_no'] == w)]['x1'].item()\n",
    "                y1 = page_df[(cond) & (page_df['word_no'] == w)]['y1'].item()\n",
    "                draw.rectangle((x0, y0, x1, y1), fill=None, outline=ImageColor.getrgb(color), width = w)\n",
    "    \"\"\"\n",
    "    for index, row in page_df.iterrows():\n",
    "        x0, y0, x1, y1 = row['x0'], row['y0'], row['x1'], row['y1'] \n",
    "        draw.rectangle((x0, y0, x1, y1), fill=None, outline=ImageColor.getrgb(color), width = w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import vol2 Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = \"../input/NOUVELLE FLORE DU LIBAN ET DE LA SYRIE 2.pdf\"\n",
    "doc = fitz.open(pdf_dir)\n",
    "pages = [doc[i] for i in range(doc.pageCount)]\n",
    "index = list(range(703, 725))\n",
    "\n",
    "TARGET_DPI = 300\n",
    "mat = fitz.Matrix(TARGET_DPI/ 72, TARGET_DPI/ 72)\n",
    "\n",
    "indent_groups = []\n",
    "indent_err = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regex based boolean functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(word):\n",
    "    \"\"\"\n",
    "    valid words are words that are:\n",
    "    - at least 2 characters\n",
    "        - unless it's x (symbol for hybrid)\n",
    "    \"\"\"\n",
    "    return (not bool(re.search(r\"[0-9]+[,.]?\", word))) and \\\n",
    "            (word != 'NOUVELLE' and word != 'FLORE') and \\\n",
    "            (word != 'INDEX' and word != 'SPECIERUM') and \\\n",
    "            (len(word) > 1 or \\\n",
    "                word == 'x' or word == 'X' or word == '×' or word == r'\\u00D7') and \\\n",
    "            ''.join(e for e in word if e.isalpha()).isalpha()\n",
    "    \n",
    "def is_genus(word):\n",
    "    \"\"\"\n",
    "    A word in the index might be a genus if it satisfies the following properties:\n",
    "    - letters: french alphabet + at most one hyphen (which is not first or last letter)\n",
    "        - first letter upper case\n",
    "        - all but first lowecase \n",
    "    in regex: ^[A-ZÀÂÄÈÉÊËÎÏÔŒÙÛÜŸÇ]{1}[a-zàâäèéêëîïôœùûüÿç]*[-]?[a-zàâäèéêëîïôœùûüÿç]+$ #ignoring strict beggining and end cause of noise\n",
    "        * based on the current expression it'd also be at least 2 letters long\n",
    "    \"\"\"\n",
    "    regex = r\"[A-ZÀÂÄÈÉÊËÎÏÔŒÙÛÜŸÇ\\u00D7]{1}[a-zàâäèéêëîïôœùûüÿç]*[-]?[a-zàâäèéêëîïôœùûüÿç]+\"\n",
    "    return re.search(regex, word)\n",
    "    \n",
    "\n",
    "def is_epithet(word):\n",
    "    \"\"\"\n",
    "    A word in the index might be an epithet if it satisfies the following properties:\n",
    "    - letters: french alphabet + at most one hyphen (which is not first or last letter)\n",
    "        - all letters lowecase \n",
    "    in regex: ^[a-zàâäèéêëîïôœùûüÿç]+[-]?[a-zàâäèéêëîïôœùûüÿç]+$ #ignoring strict beggining and end cause of noise \n",
    "        * based on the current expression it'd also be at least 2 letters long\n",
    "    \"\"\"\n",
    "    regex = r\"[a-zàâäèéêëîïôœùûüÿç\\u00D7]+[-]?[a-zàâäèéêëîïôœùûüÿç]+\"\n",
    "    return re.search(regex, word)\n",
    "    \n",
    "def is_hybrid(word):\n",
    "    regex = r\"^(([Xx\\u00D7])|([Xx\\u00D7]\\.))$\"\n",
    "    return re.search(regex, word)\n",
    "\n",
    "def is_infra(word):\n",
    "    regex = r\"^(var\\.)|(subsp\\.)\"\n",
    "    return re.search(regex, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-processing func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(page_num, indent_err = 30):\n",
    "    \n",
    "    #initiate dataframe\n",
    "    page_df = pd.DataFrame(pages[page_num].get_text_words(), columns =['in_x0', 'in_y0', 'in_x1', 'in_y1', 'word', 'block_no', 'line_no', 'word_no'])\n",
    "    \n",
    "    #add page number to dataframe\n",
    "    page_df['page_num'] = np.array([page_num]*page_df.shape[0])\n",
    "    #initiate all columns that will be added\n",
    "    page_df['genus'] = np.array([np.NaN]*page_df.shape[0])\n",
    "    page_df['draw_genus'] = np.array([np.NaN]*page_df.shape[0])\n",
    "    page_df['epithet'] = np.array([np.NaN]*page_df.shape[0])\n",
    "    page_df['draw_epithet'] = np.array([np.NaN]*page_df.shape[0])\n",
    "    page_df['author'] = np.array([np.NaN]*page_df.shape[0])\n",
    "    page_df['draw_author'] = np.array([np.NaN]*page_df.shape[0])\n",
    "    page_df['infra'] = np.array([np.NaN]*page_df.shape[0])\n",
    "    page_df['draw_infra'] = np.array([np.NaN]*page_df.shape[0])\n",
    "    page_df['taxon rank'] = np.array([np.NaN]*page_df.shape[0])\n",
    "    page_df['error_check'] = np.array([np.NaN]*page_df.shape[0])\n",
    "    #updating coordinates to represent target DPI\n",
    "    page_df['x0'], page_df['y0'], page_df['x1'], page_df['y1'] = page_df['in_x0']*TARGET_DPI/ 72, page_df['in_y0']*TARGET_DPI/ 72, page_df['in_x1']*TARGET_DPI/ 72, page_df['in_y1']*TARGET_DPI/ 72\n",
    "    #get x corner coordinates \n",
    "    x_min = page_df['x0'].min()\n",
    "    x_max = page_df['x1'].max()\n",
    "\n",
    "    y_max = page_df['y1'].max()\n",
    "\n",
    "    #Remove the extra flore - 18 at page 545\n",
    "    if page_num == index[4]:\n",
    "        page_df = page_df[~((page_df[\"word\"] == 'Flore') & (page_df['y1'] == y_max))]\n",
    "    #invalid words dataframe -- for error checking\n",
    "    pruned_words_df = page_df[~page_df[\"word\"].apply(valid)].reset_index()\n",
    "    #prune out invalid words (based on function valid)\n",
    "    page_df = page_df[page_df[\"word\"].apply(valid)].reset_index()\n",
    "    \n",
    "    indent_groups = []\n",
    "    blocks = page_df['block_no'].unique()\n",
    "    for b in blocks:\n",
    "        lines = page_df[page_df['block_no'] == b]['line_no'].unique()\n",
    "        for l in lines:\n",
    "            #reset word_no values (useful for cases where word that was originally at 0th index was pruned out)\n",
    "            cond = (page_df['line_no'] == l) & (page_df['block_no'] == b)\n",
    "            num_words = len(page_df[cond]['word_no'])\n",
    "            page_df.loc[cond, 'word_no'] = np.arange(num_words).astype(int) #this is slowww\n",
    "            #set column number (0 or 1)\n",
    "            x_0 = page_df[cond]['x0'].min()\n",
    "            #THIS DOESN'T WORK AAAA -- issue was with line no thing\n",
    "            if not np.isnan(x_0):\n",
    "                page_df.loc[cond, 'col_no'] = np.array([int(x_0 > ((x_min + x_max) / 2))]*num_words).astype(int)\n",
    "\n",
    "                #initiate indent groups -- only first word should get an indent_group value \n",
    "                new_group = True\n",
    "                for g_i in range(len(indent_groups)):\n",
    "                    g = indent_groups[g_i]\n",
    "                    g_arr = np.array(g)\n",
    "                    if x_0 <= np.mean(g_arr) + indent_err and x_0 >= np.mean(g_arr) - indent_err:\n",
    "                        g.append(x_0)\n",
    "                        new_group = False\n",
    "                        page_df.loc[cond, 'indent_group'] = np.array([g_i]*num_words).astype(int)\n",
    "                if new_group:\n",
    "                    indent_groups.append([x_0])\n",
    "                    g_i = len(indent_groups) - 1\n",
    "                    page_df.loc[cond, 'indent_group'] = np.array([g_i]*num_words).astype(int)\n",
    "\n",
    "    #print(\"indent groups:\", indent_groups)\n",
    "    #return updated page_df, pruned_words_df, indent groups\n",
    "    return page_df.reset_index(), pruned_words_df, indent_groups\n",
    "\n",
    "#https://stackoverflow.com/questions/53468558/adding-image-to-pandas-dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding indentations associated with genus, epithet, infra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['genus', 'epithet', 'infra', 'author', 'misc.']\n",
    "def n_leftmost_indent(df, n):\n",
    "    \"\"\"return a tuple with at most 3 elements each element itself is a tuple containing indent group, mean, group len\"\"\"\n",
    "    indent_groups = [(g, df[(df['indent_group'] == g) & (df['word_no'] == 0)]['x0'].mean(), len(df[(df['indent_group'] == g) & (df['word_no'] == 0)]['x0'])) for g in df['indent_group'].unique()]\n",
    "    indent_groups.sort(key = lambda x : x[1])\n",
    "    #print(indent_groups[:n])\n",
    "    return indent_groups[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genusEpithetInfra_indent(col_df):\n",
    "    leftmost_3_indents = n_leftmost_indent(col_df, 3) #for vol1 only 2 indentations will be given \n",
    "    min_gap = 0\n",
    "    max_gap = 75 #error is 30 -- less than 50% of max gap (which will be ignored for now)\n",
    "\n",
    "    # possibly not specific enough\n",
    "    # first identifying indent based don distance from one another only\n",
    "    \"\"\"if len(leftmost_3_indents) == 3:\n",
    "        if leftmost_3_indents[0][1] < max_gap:\n",
    "            leftmost_3_indents = leftmost_3_indents[1:]\n",
    "        elif ((leftmost_3_indents[1][1] - leftmost_3_indents[0][1]) > max_gap or \\\n",
    "            (leftmost_3_indents[1][1] - leftmost_3_indents[0][1]) < min_gap): #comparing first two (if satisfied last two will be checked in next if block)\n",
    "            leftmost_3_indents = [max(leftmost_3_indents[1:], key = lambda x : x[2])] + [leftmost_3_indents[2]]\n",
    "        elif (leftmost_3_indents[2][1] - leftmost_3_indents[1][1]) > max_gap or \\\n",
    "            (leftmost_3_indents[2][1] - leftmost_3_indents[1][1]) < min_gap: #comparing last two\n",
    "            leftmost_3_indents = [leftmost_3_indents[0]] + [max(leftmost_3_indents[1:], key = lambda x : x[2])]\n",
    "\n",
    "    if len(leftmost_3_indents) == 2:\n",
    "        if leftmost_3_indents[0][1] < max_gap:\n",
    "            leftmost_3_indents = leftmost_3_indents[1]\n",
    "        elif (leftmost_3_indents[1][1] - leftmost_3_indents[0][1]) > max_gap or (leftmost_3_indents[1][1] - leftmost_3_indents[0][1]) < min_gap:\n",
    "            leftmost_3_indents = [max(leftmost_3_indents, key = lambda x : x[2])]\"\"\"\n",
    "\n",
    "    has_genus, has_epithet, has_infra = False, False, False\n",
    "    genus_indent, epithet_indent, infra_indent = -1, -1, -1\n",
    "    if len(leftmost_3_indents) == 3 and type(leftmost_3_indents) == type([1,2,3]):\n",
    "        has_genus, has_epithet, has_infra = True, True, True\n",
    "        #print(\"leftmost 3:\", leftmost_3_indents)\n",
    "        genus_indent, epithet_indent, infra_indent = [el[0] for el in leftmost_3_indents]\n",
    "    elif len(leftmost_3_indents) == 2:\n",
    "        if col_df[col_df['indent_group'] == leftmost_3_indents[1][0]]['word'].apply(is_infra).any():\n",
    "            has_genus, has_epithet, has_infra = False, True, True\n",
    "            epithet_indent, infra_indent = [el[0] for el in leftmost_3_indents]\n",
    "        else:\n",
    "            has_genus, has_epithet, has_infra = True, True, False\n",
    "            genus_indent, epithet_indent = [el[0] for el in leftmost_3_indents]\n",
    "    elif len(leftmost_3_indents) == 1 or type(leftmost_3_indents) == type((1,2,3)): \n",
    "        if type(leftmost_3_indents) == type((1,2,3)):\n",
    "            leftmost_3_indents = [leftmost_3_indents]\n",
    "        has_genus, has_epithet, has_infra = False, True, False\n",
    "        epithet_indent = leftmost_3_indents[0][0]\n",
    "\n",
    "    return genus_indent, epithet_indent, infra_indent, leftmost_3_indents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing column dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_col(col_df, genus, epithet, draw_genus, draw_epithet, draw_infra = np.NaN):\n",
    "    genus_indent, epithet_indent, infra_indent, indent_3_left = get_genusEpithetInfra_indent(col_df)\n",
    "    #print(genus_indent, epithet_indent, infra_indent, indent_3_left)\n",
    "    \n",
    "    blocks = col_df['block_no'].unique()\n",
    "    start_word_cond = -1 \n",
    "    author = ''\n",
    "    #draw_infra = np.NaN\n",
    "\n",
    "    process_hybrid = False\n",
    "    process_infra = False\n",
    "    hybrid_pre_str = ''\n",
    "\n",
    "    col_df = col_df.copy()\n",
    "    for index, row in col_df.iterrows():\n",
    "        b, l, w = row['block_no'], row['line_no'], row['word_no']\n",
    "        word, indent_group = row['word'], row['indent_group']\n",
    "        row_cond = (col_df['line_no'] == l) & (col_df['block_no'] == b) & (col_df['word_no'] == w) \n",
    "        \n",
    "        if w == 0 or process_hybrid: \n",
    "            if is_hybrid(word):\n",
    "                process_hybrid = True\n",
    "                misc = word\n",
    "                author = ''\n",
    "                hybrid_pre_str = 'hybrid-'\n",
    "            \n",
    "            else:\n",
    "                if process_hybrid: \n",
    "                    #col_df.loc[start_word_cond, 'misc.'] = f\"hybrid ({misc})\"\n",
    "                    process_hybrid = False\n",
    "                    hybrid_pre_str = 'hybrid-'\n",
    "                    #print(\"hi\")\n",
    "                else: \n",
    "                    hybrid_pre_str = ''\n",
    "\n",
    "                prev_start_cond = start_word_cond\n",
    "                start_word_cond = row_cond\n",
    "                process_hybrid = False\n",
    "                process_infra = False\n",
    "                \n",
    "                if indent_group == genus_indent and not ''.join(e for e in word if e.isalpha()).isupper():\n",
    "                    process_infra = False\n",
    "                    genus = word\n",
    "                    draw_genus = genus\n",
    "                    epithet = ''\n",
    "                    draw_epithet = ''\n",
    "                    author = ''\n",
    "                    misc = ''\n",
    "                    infra = ''\n",
    "                    col_df.loc[start_word_cond, 'genus'] = genus\n",
    "                    col_df.loc[start_word_cond, 'taxon rank'] = hybrid_pre_str  + 'genus'\n",
    "                    if not is_genus(word):\n",
    "                        col_df.loc[row_cond, 'error_check'] = True\n",
    "                    col_df.loc[row_cond, 'draw_genus'] = draw_genus\n",
    "                    col_df.loc[row_cond, 'author'] = ''\n",
    "\n",
    "                elif indent_group == epithet_indent and not ''.join(e for e in word if e.isalpha()).isupper():\n",
    "                    process_infra = False\n",
    "                    epithet = word\n",
    "                    author = ''\n",
    "                    col_df.loc[row_cond, 'genus'] = genus\n",
    "                    col_df.loc[row_cond, 'epithet'] = epithet\n",
    "                    col_df.loc[row_cond, 'taxon rank'] = hybrid_pre_str  + 'species'\n",
    "                    if not is_epithet(word):\n",
    "                        col_df.loc[row_cond, 'error_check'] = True\n",
    "                    draw_epithet = str(genus) + '_' + str(epithet) +'_' + str(b) + '_' + str(l)\n",
    "                    col_df.loc[row_cond, 'draw_genus'] = draw_genus\n",
    "                    col_df.loc[row_cond, 'draw_epithet'] = draw_epithet\n",
    "                    col_df.loc[row_cond, 'author'] = ''\n",
    "\n",
    "                elif indent_group == infra_indent and is_infra:\n",
    "                    #col_df.loc[row_cond, 'misc.'] = word\n",
    "                    process_infra = True\n",
    "                    misc = word\n",
    "                    author = ''\n",
    "                    if not (is_infra(word) or is_hybrid(word)):\n",
    "                        col_df.loc[start_word_cond, 'error_check'] = True\n",
    "                        #possibly should add to author of previous line here? \n",
    "                        #print(\"possible author?\", word)\n",
    "                    #print(\"hi\")\n",
    "\n",
    "        elif w == 1 and process_infra and (is_infra(misc) or len(''.join(e for e in misc if e.isalpha())) <2) and (misc != \"et\"):\n",
    "            infra = word \n",
    "            start_word_cond = row_cond\n",
    "            #print(updated)\n",
    "            #print(b, l, w)\n",
    "            #col_df.loc[row_cond, 'misc.'] = misc\n",
    "            col_df.loc[start_word_cond, 'taxon rank'] = hybrid_pre_str + misc\n",
    "            col_df.loc[start_word_cond, 'genus'] = genus\n",
    "            col_df.loc[start_word_cond, 'epithet'] = epithet\n",
    "            col_df.loc[start_word_cond, 'infra'] = infra\n",
    "            col_df.loc[start_word_cond, 'author'] = ''\n",
    "            draw_infra = str(infra) + '_'+str(b)+'_'+str(l)\n",
    "            process_infra = False\n",
    "            col_df.loc[start_word_cond, 'draw_genus'] = draw_genus\n",
    "            col_df.loc[start_word_cond, 'draw_epithet'] = draw_epithet\n",
    "            col_df.loc[start_word_cond, 'draw_infra'] = draw_infra\n",
    "        \n",
    "        elif process_infra:\n",
    "            #print(misc, word)\n",
    "            if misc != \"\":\n",
    "                curr_author_part = misc + ' ' + word + ' '\n",
    "                col_df.loc[start_word_cond, 'draw_author'] = 'author_'+str(b)+'_'+str(l)\n",
    "            else:\n",
    "                curr_author_part = word + ' '\n",
    "            col_df.loc[prev_start_cond, 'author'] += curr_author_part\n",
    "            col_df.loc[row_cond, 'draw_author'] = 'author_'+str(b)+'_'+str(l)\n",
    "            col_df.loc[row_cond, 'draw_genus'] = draw_genus\n",
    "\n",
    "        elif (type(genus) == type(\"STR\") and genus != '') or (type(epithet) == type(\"STR\") and epithet != ''):\n",
    "            curr_author_part = word + ' '\n",
    "            col_df.loc[start_word_cond, 'author'] += curr_author_part\n",
    "            col_df.loc[row_cond, 'draw_author'] = 'author_'+str(b)+'_'+str(l)\n",
    "            col_df.loc[row_cond, 'draw_genus'] = draw_genus\n",
    "\n",
    "    #Last author\n",
    "    \"\"\"if author != '':\n",
    "        col_df.loc[start_word_cond, 'author'] = author\"\"\"\n",
    "                    \n",
    "\n",
    "    return col_df, genus, epithet, draw_genus, draw_epithet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:08<00:00,  2.72it/s]\n",
      "100%|██████████| 22/22 [00:07<00:00,  3.08it/s]\n"
     ]
    }
   ],
   "source": [
    "#preprocessing\n",
    "genus = np.NaN\n",
    "df_dict = {}\n",
    "pruned_dict = {}\n",
    "\n",
    "for page_num in tqdm(index):\n",
    "    page_df, pruned_df, indent_group = preprocessing(page_num)\n",
    "    df_dict[page_num] = page_df\n",
    "    pruned_dict[page_num] = pruned_df\n",
    "\n",
    "genus = np.NaN\n",
    "epithet = np.NaN\n",
    "draw_genus = np.NaN\n",
    "draw_epithet = np.NaN\n",
    "result_ims_valid_words = []\n",
    "df_list = []\n",
    "\n",
    "for page_num in tqdm(index):\n",
    "    #page_num = index[-1]\n",
    "    #process the pre-processed dfs\n",
    "    page_df = df_dict[page_num]\n",
    "    \n",
    "    #for drawing\n",
    "    pix_map = doc.get_page_pixmap(page_num,matrix=mat)\n",
    "    image = Image.open(io.BytesIO(pix_map.tobytes()))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    plot_valid_words(page_df, draw, color = '#660044', w = 2)\n",
    "    result_ims_valid_words.append(image)\n",
    "    \n",
    "    #break \n",
    "#result_ims_valid_words[0].save(OUTPUT_PATH + \"preprocessed/\" + 'valid_words' + TAIL_STR + '.pdf',save_all=True, append_images=result_ims[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results + SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up files and directories for saving the results\n",
    "SCRIPT_NAME = \"reverted_index_Vol2_reorganized\"\n",
    "SCRIPT_OUTPUT_PATH = \"../output/index/\" + SCRIPT_NAME + \"/\"\n",
    "DATE_STR = datetime.now().strftime(\"%Y_%m_%d\") \n",
    "TIME_STR = datetime.now().strftime(\"%H%M\")\n",
    "QUICK_FIX = False\n",
    "TAIL_STR = ''\n",
    "\n",
    "if QUICK_FIX:\n",
    "    OUTPUT_PATH = SCRIPT_OUTPUT_PATH + DATE_STR + \"/QuickFix/\" \n",
    "    #TAIL_STR = '_' + DATE_STR + '_' + TIME_STR\n",
    "else:\n",
    "    OUTPUT_PATH = SCRIPT_OUTPUT_PATH + DATE_STR + \"/\" + TIME_STR + \"/\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "except FileExistsError:\n",
    "    # directory already exists\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.makedirs(OUTPUT_PATH + \"preprocessed/\")\n",
    "except FileExistsError:\n",
    "    # directory already exists\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.makedirs(OUTPUT_PATH + 'raw/')\n",
    "except FileExistsError:\n",
    "    # directory already exists\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:22<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "genus = np.NaN\n",
    "epithet = np.NaN\n",
    "draw_genus = np.NaN\n",
    "draw_epithet = np.NaN\n",
    "result_ims = []\n",
    "df_list = []\n",
    "\n",
    "for page_num in tqdm(index):\n",
    "    #if page_num == index[-2]:\n",
    "    #    break\n",
    "    #page_num = index[-1]\n",
    "    #process the pre-processed dfs\n",
    "    page_df = df_dict[page_num]\n",
    "    \n",
    "    #for drawing\n",
    "    pix_map = doc.get_page_pixmap(page_num,matrix=mat)\n",
    "    image = Image.open(io.BytesIO(pix_map.tobytes()))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    #processing each column\n",
    "    for c in page_df['col_no'].unique():\n",
    "        col_df = page_df[page_df['col_no'] == c]\n",
    "        col_df, genus, epithet, draw_genus, draw_epithet = process_col(col_df, genus, epithet, draw_genus, draw_epithet)\n",
    "        df_list.append(col_df)\n",
    "\n",
    "        #drawing boxes in each column\n",
    "        plot_genus_blocks(col_df, draw)\n",
    "        plot_epithet_blocks(col_df, draw)\n",
    "        plot_author_blocks(col_df, draw)\n",
    "        plot_infra_blocks(col_df, draw)\n",
    "\n",
    "    result_ims.append(image)\n",
    "    #break \n",
    "\n",
    "#TIME_STR = datetime.now().strftime(\"%Y_%m_%d-%I_%M_%p\")\n",
    "result_ims[0].save(OUTPUT_PATH + 'vol2_index_ROI.pdf',save_all=True, append_images=result_ims[1:])\n",
    "\n",
    "pre_processed_df = pd.concat([df_dict[k] for k in df_dict], axis = 0)\n",
    "result_ims_valid_words[0].save(OUTPUT_PATH + \"preprocessed/\" + 'valid_words' + TAIL_STR + '.pdf',save_all=True, append_images=result_ims[1:])\n",
    "pre_processed_df.to_html(OUTPUT_PATH + \"preprocessed/\" + 'vol2_preprocessed_index' + TAIL_STR + '.html')\n",
    "pre_processed_df.to_csv(OUTPUT_PATH + \"preprocessed/\" + 'vol2_preprocessed_index' + TAIL_STR + '.csv')\n",
    "\n",
    "df = pd.concat(df_list, axis = 0)\n",
    "df.to_html(OUTPUT_PATH + 'raw/' + 'vol2_index' + TAIL_STR + '.html')\n",
    "df.to_csv(OUTPUT_PATH + 'raw/' + 'vol2_index' + TAIL_STR + '.csv', index = False)\n",
    "\n",
    "pruned = df[(~df['genus'].isnull())]\n",
    "pruned = pruned[[\"page_num\", \"genus\", \"epithet\", \"infra\" ,\"author\", \"taxon rank\"]]\n",
    "pruned.to_csv(OUTPUT_PATH + 'vol2_index_pruned' + TAIL_STR + '.csv', index = False)\n",
    "pruned.to_html(OUTPUT_PATH + 'vol2_index_pruned' + TAIL_STR + '.html')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}