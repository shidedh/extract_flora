{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import fitz\n",
    "import re\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageColor\n",
    "import operator\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROI Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_genus_blocks(page_df, draw, color = '#6c899e', w = 3):\n",
    "    try:\n",
    "        genus_list = page_df['draw_genus'].unique()\n",
    "    except:\n",
    "        print(\"no GENUS found\")\n",
    "        return \n",
    "\n",
    "    for g in genus_list:\n",
    "        temp_df = page_df[(page_df['draw_genus'] == g)]\n",
    "        g_x0 = temp_df['x0'].min()\n",
    "        g_y0 = temp_df['y0'].min()\n",
    "        g_x1 = temp_df['x1'].max()\n",
    "        g_y1 = temp_df['y1'].max()\n",
    "\n",
    "        draw.rectangle((g_x0, g_y0, g_x1, g_y1), fill=None, outline=ImageColor.getrgb(color), width = w)\n",
    "        \n",
    "def plot_epithet_blocks(page_df, draw, color = '#660066', w = 3):\n",
    "    try:\n",
    "        epithet_list = page_df['draw_epithet'].unique()\n",
    "    except:\n",
    "        print(\"no EPITHET found\")\n",
    "        return \n",
    "    \n",
    "    for e in epithet_list:\n",
    "        temp_df = page_df[(page_df['draw_epithet'] == e)]\n",
    "        e_x0 = temp_df['x0'].min()\n",
    "        e_y0 = temp_df['y0'].min()\n",
    "        e_x1 = temp_df['x1'].max()\n",
    "        e_y1 = temp_df['y1'].max()\n",
    "\n",
    "        draw.rectangle((e_x0, e_y0, e_x1, e_y1), fill=None, outline=ImageColor.getrgb(color), width = w)\n",
    "\n",
    "def plot_author_blocks(page_df, draw, color = '#a3a3a3', w = 1):\n",
    "    try:\n",
    "        author_list = page_df['draw_author'].unique()\n",
    "    except:\n",
    "        print(\"no AUTHOR found\")\n",
    "        return \n",
    "\n",
    "    for a in author_list:\n",
    "        temp_df = page_df[(page_df['draw_author'] == a)]\n",
    "        e_x0 = temp_df['x0'].min()\n",
    "        e_y0 = temp_df['y0'].min()\n",
    "        e_x1 = temp_df['x1'].max()\n",
    "        e_y1 = temp_df['y1'].max()\n",
    "\n",
    "        draw.rectangle((e_x0, e_y0, e_x1, e_y1), fill=None, outline=ImageColor.getrgb(color), width = w)\n",
    "\n",
    "def plot_infra_blocks(page_df, draw, color = '#ff6289', w = 1):\n",
    "    try:\n",
    "        infra_list = page_df['draw_infra'].unique()\n",
    "    except:\n",
    "        print(\"no INFRA Spp. found\")\n",
    "        return \n",
    "\n",
    "    for infra_spp in infra_list:\n",
    "        temp_df = page_df[(page_df['draw_infra'] == infra_spp)]\n",
    "        e_x0 = temp_df['x0'].min()\n",
    "        e_y0 = temp_df['y0'].min()\n",
    "        e_x1 = temp_df['x1'].max()\n",
    "        e_y1 = temp_df['y1'].max()\n",
    "\n",
    "        draw.rectangle((e_x0, e_y0, e_x1, e_y1), fill=None, outline=ImageColor.getrgb(color), width = w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regex based boolean functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(word):\n",
    "    \"\"\"\n",
    "    valid words are words that are:\n",
    "    - at least 2 characters\n",
    "        - unless it's x (symbol for hybrid)\n",
    "    \"\"\"\n",
    "    return (not bool(re.search(r\"[0-9]+[,.]?\", word))) and (len(word) > 1 or word == 'x' or word == 'X' or word == '×' or word == r'\\u00D7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_genus(word):\n",
    "    \"\"\"\n",
    "    A word in the index might be a genus if it satisfies the following properties:\n",
    "    - letters: french alphabet + at most one hyphen (which is not first or last letter)\n",
    "        - first letter upper case\n",
    "        - all but first lowecase \n",
    "    in regex: ^[A-ZÀÂÄÈÉÊËÎÏÔŒÙÛÜŸÇ]{1}[a-zàâäèéêëîïôœùûüÿç]*[-]?[a-zàâäèéêëîïôœùûüÿç]+$ #ignoring strict beggining and end cause of noise\n",
    "        * based on the current expression it'd also be at least 2 letters long\n",
    "    \"\"\"\n",
    "    regex = r\"[A-ZÀÂÄÈÉÊËÎÏÔŒÙÛÜŸÇ\\u00D7]{1}[a-zàâäèéêëîïôœùûüÿç]*[-]?[a-zàâäèéêëîïôœùûüÿç]+\"\n",
    "    return re.search(regex, word)\n",
    "    \n",
    "\n",
    "def is_epithet(word):\n",
    "    \"\"\"\n",
    "    A word in the index might be an epithet if it satisfies the following properties:\n",
    "    - letters: french alphabet + at most one hyphen (which is not first or last letter)\n",
    "        - all letters lowecase \n",
    "    in regex: ^[a-zàâäèéêëîïôœùûüÿç]+[-]?[a-zàâäèéêëîïôœùûüÿç]+$ #ignoring strict beggining and end cause of noise \n",
    "        * based on the current expression it'd also be at least 2 letters long\n",
    "    \"\"\"\n",
    "    regex = r\"[a-zàâäèéêëîïôœùûüÿç\\u00D7]+[-]?[a-zàâäèéêëîïôœùûüÿç]+\"\n",
    "    return re.search(regex, word)\n",
    "    \n",
    "def is_hybrid(word):\n",
    "    regex = r\"^(([Xx\\u00D7])|([Xx\\u00D7]\\.))$\"\n",
    "    return re.search(regex, word)\n",
    "\n",
    "def is_infra(word):\n",
    "    regex = r\"^(var\\.)|(subsp\\.)\"\n",
    "    return re.search(regex, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(pages, page_num, indent_err = 15):\n",
    "    \n",
    "    #initiate dataframe\n",
    "    page_df = pd.DataFrame(pages[page_num].get_text_words(), columns =['in_x0', 'in_y0', 'in_x1', 'in_y1', 'word', 'block_no', 'line_no', 'word_no'])\n",
    "    \n",
    "    #add page number to dataframe\n",
    "    page_df['page_num'] = np.array([page_num]*page_df.shape[0])\n",
    "    #updating coordinates to represent target DPI\n",
    "    page_df['x0'], page_df['y0'], page_df['x1'], page_df['y1'] = page_df['in_x0']*TARGET_DPI/ 72, page_df['in_y0']*TARGET_DPI/ 72, page_df['in_x1']*TARGET_DPI/ 72, page_df['in_y1']*TARGET_DPI/ 72\n",
    "    #get x corner coordinates \n",
    "    x_min = page_df['x0'].min()\n",
    "    x_max = page_df['x1'].max()\n",
    "\n",
    "    #invalid words dataframe -- for error checking\n",
    "    pruned_words_df = page_df[~page_df[\"word\"].apply(valid)].reset_index()\n",
    "    #prune out invalid words (based on function valid)\n",
    "    page_df = page_df[page_df[\"word\"].apply(valid)].reset_index()\n",
    "    \n",
    "    indent_groups = []\n",
    "    blocks = page_df['block_no'].unique()\n",
    "    for b in blocks:\n",
    "        lines = page_df[page_df['block_no'] == b]['line_no'].unique()\n",
    "        for l in lines:\n",
    "            #reset word_no values (useful for cases where word that was originally at 0th index was pruned out)\n",
    "            cond = (page_df['line_no'] == l) & (page_df['block_no'] == b)\n",
    "            num_words = len(page_df[cond]['word_no'])\n",
    "            page_df.loc[cond, 'word_no'] = np.arange(num_words).astype(int) #this is slowww\n",
    "            #set column number (0 or 1)\n",
    "            x_0 = page_df[cond]['x0'].min()\n",
    "            #THIS DOESN'T WORK AAAA -- issue was with line no thing\n",
    "            if not np.isnan(x_0):\n",
    "                page_df.loc[cond, 'col_no'] = np.array([int(x_0 > ((x_min + x_max) / 2))]*num_words).astype(int)\n",
    "\n",
    "                #initiate indent groups -- only first word should get an indent_group value \n",
    "                new_group = True\n",
    "                for g_i in range(len(indent_groups)):\n",
    "                    g = indent_groups[g_i]\n",
    "                    g_arr = np.array(g)\n",
    "                    if x_0 <= np.mean(g_arr) + indent_err and x_0 >= np.mean(g_arr) - indent_err:\n",
    "                        g.append(x_0)\n",
    "                        new_group = False\n",
    "                        page_df.loc[cond, 'indent_group'] = np.array([g_i]*num_words).astype(int)\n",
    "                if new_group:\n",
    "                    indent_groups.append([x_0])\n",
    "                    g_i = len(indent_groups) - 1\n",
    "                    page_df.loc[cond, 'indent_group'] = np.array([g_i]*num_words).astype(int)\n",
    "\n",
    "\n",
    "    #return updated page_df, pruned_words_df, indent groups\n",
    "    return page_df.reset_index(), pruned_words_df, indent_groups\n",
    "\n",
    "#https://stackoverflow.com/questions/53468558/adding-image-to-pandas-dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding indentations associated with genus, epithet, infra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['genus', 'epithet', 'infra', 'author', 'misc.']\n",
    "def n_leftmost_indent(df, n):\n",
    "    \"\"\"return a tuple with at most 3 elements each element itself is a tuple containing indent group, mean, group len\"\"\"\n",
    "    indent_groups = [(g, df[(df['indent_group'] == g) & (df['word_no'] == 0)]['x0'].mean(), len(df[(df['indent_group'] == g) & (df['word_no'] == 0)]['x0'])) for g in df['indent_group'].unique()]\n",
    "    indent_groups.sort(key = lambda x : x[1])\n",
    "    return indent_groups[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_genusEpithetInfra_indent(col_df):\n",
    "    leftmost_3_indents = n_leftmost_indent(col_df, 3) \n",
    "    min_gap = 25\n",
    "    max_gap = 50\n",
    "\n",
    "    # possibly not specific enough\n",
    "    # first identifying indent based don distance from one another only\n",
    "    if len(leftmost_3_indents) == 3:\n",
    "        if leftmost_3_indents[0][1] < max_gap:\n",
    "            leftmost_3_indents = leftmost_3_indents[1:]\n",
    "        elif ((leftmost_3_indents[1][1] - leftmost_3_indents[0][1]) > max_gap or \\\n",
    "            (leftmost_3_indents[1][1] - leftmost_3_indents[0][1]) < min_gap): #comparing first two (if satisfied last two will be checked in next if block)\n",
    "            leftmost_3_indents = [max(leftmost_3_indents[1:], key = lambda x : x[2])] + [leftmost_3_indents[2]]\n",
    "        elif (leftmost_3_indents[2][1] - leftmost_3_indents[1][1]) > max_gap or \\\n",
    "            (leftmost_3_indents[2][1] - leftmost_3_indents[1][1]) < min_gap: #comparing last two\n",
    "            leftmost_3_indents = [leftmost_3_indents[0]] + [max(leftmost_3_indents[1:], key = lambda x : x[2])]\n",
    "\n",
    "    if len(leftmost_3_indents) == 2:\n",
    "        if leftmost_3_indents[0][1] < max_gap:\n",
    "            leftmost_3_indents = leftmost_3_indents[1]\n",
    "        elif (leftmost_3_indents[1][1] - leftmost_3_indents[0][1]) > max_gap or (leftmost_3_indents[1][1] - leftmost_3_indents[0][1]) < min_gap:\n",
    "            leftmost_3_indents = [max(leftmost_3_indents, key = lambda x : x[2])]\n",
    "\n",
    "    has_genus, has_epithet, has_infra = False, False, False\n",
    "    genus_indent, epithet_indent, infra_indent = -1, -1, -1\n",
    "    if len(leftmost_3_indents) == 3:\n",
    "        has_genus, has_epithet, has_infra = True, True, True\n",
    "        genus_indent, epithet_indent, infra_indent = [el[0] for el in leftmost_3_indents]\n",
    "    elif len(leftmost_3_indents) == 2:\n",
    "        if col_df[col_df['indent_group'] == leftmost_3_indents[1][0]]['word'].apply(is_infra).any():\n",
    "            has_genus, has_epithet, has_infra = False, True, True\n",
    "            epithet_indent, infra_indent = [el[0] for el in leftmost_3_indents]\n",
    "        else:\n",
    "            has_genus, has_epithet, has_infra = True, True, False\n",
    "            genus_indent, epithet_indent = [el[0] for el in leftmost_3_indents]\n",
    "    elif len(leftmost_3_indents) == 2: \n",
    "        has_genus, has_epithet, has_infra = False, True, False\n",
    "        epithet_indent = leftmost_3_indents[0][0]\n",
    "\n",
    "    return genus_indent, epithet_indent, infra_indent, leftmost_3_indents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing column dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_col(col_df, genus, epithet, draw_genus, draw_epithet, draw_infra = np.NaN):\n",
    "    genus_indent, epithet_indent, infra_indent, indent_3_left = get_genusEpithetInfra_indent(col_df)\n",
    "    blocks = col_df['block_no'].unique()\n",
    "    start_word_cond = -1 \n",
    "    author = ''\n",
    "    #draw_infra = np.NaN\n",
    "    \n",
    "    for b in blocks:\n",
    "        lines = col_df[col_df['block_no'] == b]['line_no'].unique()\n",
    "        for l in lines:\n",
    "            cond = (col_df['line_no'] == l) & (col_df['block_no'] == b)\n",
    "            words = col_df[cond]['word_no'].unique()\n",
    "            process_hybrid = False\n",
    "            process_infra = False\n",
    "            \n",
    "            col_df = col_df.copy()\n",
    "            for w in words:\n",
    "                word_cond = (col_df['line_no'] == l) & (col_df['block_no'] == b) & (col_df['word_no'] == w) \n",
    "                word = col_df[word_cond]['word'].item()\n",
    "                #print(word)\n",
    "            \n",
    "                if w == 0:\n",
    "                    infra = ''\n",
    "                    if author != '':\n",
    "                        col_df.loc[start_word_cond, 'author'] = author\n",
    "                        author = ''\n",
    "                    \n",
    "                    start_word_cond = word_cond\n",
    "                    start_l = l \n",
    "                    start_b = b \n",
    "\n",
    "                    indent_group = col_df[word_cond]['indent_group'].item()\n",
    "                    \n",
    "                    if is_hybrid(word):\n",
    "                        process_hybrid = True\n",
    "                        misc = word\n",
    "                        author = ''\n",
    "                        #col_df.loc[start_word_cond, 'misc.'] = misc\n",
    "                    #now only gotta say INDENT AND satisfies these paterns\n",
    "                    #print(indent_group, genus_indent)\n",
    "                    else: \n",
    "                        if indent_group == genus_indent:\n",
    "                            if not ''.join(e for e in word if e.isalpha()).isupper():\n",
    "                                genus = word\n",
    "                                misc = ''\n",
    "                                author = ''\n",
    "                                infra = ''\n",
    "                                epithet = ''\n",
    "                                draw_genus = genus\n",
    "                                col_df.loc[start_word_cond, 'genus'] = genus\n",
    "                                col_df.loc[start_word_cond, 'taxon rank'] = 'genus'\n",
    "                                if not is_genus(word):\n",
    "                                    col_df.loc[start_word_cond, 'error_check'] = True\n",
    "                                \n",
    "                            else: \n",
    "                                genus = ''\n",
    "                                misc = ''\n",
    "                                author = ''\n",
    "                                infra = ''\n",
    "                                epithet = ''\n",
    "                                draw_genus = ''\n",
    "                        elif indent_group == epithet_indent:\n",
    "                            epithet = word\n",
    "                            misc = ''\n",
    "                            infra = ''\n",
    "                            author = ''\n",
    "                            col_df.loc[start_word_cond, 'genus'] = genus\n",
    "                            col_df.loc[start_word_cond, 'epithet'] = epithet\n",
    "                            col_df.loc[start_word_cond, 'taxon rank'] = 'species'\n",
    "                            if not is_epithet(word):\n",
    "                                col_df.loc[start_word_cond, 'error_check'] = True\n",
    "                            draw_epithet = str(genus) + '_' + str(epithet) +'_' + str(start_b) + '_' + str(start_l)\n",
    "                            \n",
    "                        elif indent_group == infra_indent:\n",
    "                            process_infra = True\n",
    "                            misc = word\n",
    "                            author = ''\n",
    "                            #col_df.loc[start_word_cond, 'misc.'] = misc\n",
    "                            if not (is_infra(word) or is_hybrid(word)):\n",
    "                                col_df.loc[start_word_cond, 'error_check'] = True\n",
    "                    \n",
    "                elif process_infra:\n",
    "                    start_word_cond = word_cond\n",
    "                    start_l = l \n",
    "                    start_b = b \n",
    "                    infra = word \n",
    "                    col_df.loc[start_word_cond, 'genus'] = genus\n",
    "                    col_df.loc[start_word_cond, 'epithet'] = epithet\n",
    "                    col_df.loc[start_word_cond, 'infra'] = infra\n",
    "                    #col_df.loc[start_word_cond, 'misc.'] = misc\n",
    "                    col_df.loc[start_word_cond, 'taxon rank'] = misc\n",
    "                    draw_infra = str(infra) + '_'+str(start_b)+'_'+str(start_l)\n",
    "                    process_infra = False\n",
    "                    \n",
    "                elif process_hybrid:\n",
    "                    start_word_cond = word_cond\n",
    "                    start_l = l \n",
    "                    start_b = b \n",
    "                    if indent_group == genus_indent:\n",
    "                        genus = word\n",
    "                        epithet = ''\n",
    "                        infra = ''\n",
    "                        author = ''\n",
    "                        draw_genus = genus\n",
    "                        col_df.loc[start_word_cond, 'genus'] = genus\n",
    "                        col_df.loc[start_word_cond, 'taxon rank'] = 'genus - hybrid'\n",
    "                        if not is_genus(word):\n",
    "                            col_df.loc[start_word_cond, 'error_check'] = True\n",
    "                            \n",
    "                    elif indent_group == epithet_indent:\n",
    "                        epithet = word\n",
    "                        author = ''\n",
    "                        infra = ''\n",
    "                        col_df.loc[start_word_cond, 'genus'] = genus\n",
    "                        col_df.loc[start_word_cond, 'epithet'] = epithet\n",
    "                        col_df.loc[start_word_cond, 'taxon rank'] = 'species - hybrid'\n",
    "                        draw_epithet = str(genus) + '_' + str(epithet) +'_' + str(start_b) + '_' + str(start_l)\n",
    "                        if not is_epithet(word):\n",
    "                            col_df.loc[start_word_cond, 'error_check'] = True\n",
    "                    elif indent_group == infra_indent:\n",
    "                        infra = word\n",
    "                        col_df.loc[start_word_cond, 'genus'] = genus\n",
    "                        col_df.loc[start_word_cond, 'epithet'] = epithet\n",
    "                        col_df.loc[start_word_cond, 'infra'] = infra\n",
    "                        col_df.loc[start_word_cond, 'taxon rank'] = 'hybrid'\n",
    "\n",
    "                    #col_df.loc[start_word_cond, 'misc.'] = 'x'\n",
    "                    process_hybrid = False\n",
    "                else:\n",
    "                    author = author + word + ' '\n",
    "                    col_df.loc[word_cond, 'draw_author'] = 'author_'+str(start_b)+'_'+str(start_l)\n",
    "                \n",
    "                if genus:\n",
    "                    col_df.loc[word_cond, 'draw_genus'] = draw_genus\n",
    "                if epithet:\n",
    "                    col_df.loc[word_cond, 'draw_epithet'] = draw_epithet\n",
    "                if infra: \n",
    "                    col_df.loc[word_cond, 'draw_infra'] = draw_infra\n",
    "\n",
    "    #Last author\n",
    "    if author != '':\n",
    "        col_df.loc[start_word_cond, 'author'] = author\n",
    "    \n",
    "    return col_df, genus, epithet, draw_genus, draw_epithet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing \n",
    "def preprocessing_pages(pages, index):\n",
    "    pre_df_dict = {}\n",
    "    pruned_dict = {}\n",
    "\n",
    "    for page_num in tqdm(index):\n",
    "        #print(page_num, type(page_num))\n",
    "        page_df, pruned_df, indent_group = preprocessing(pages, page_num)\n",
    "        pre_df_dict[page_num] = page_df\n",
    "        pruned_dict[page_num] = pruned_df\n",
    "    return pre_df_dict, pruned_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing columns \n",
    "def processing_pages(pre_df_dict, index):\n",
    "    genus = np.NaN\n",
    "    epithet = np.NaN\n",
    "    draw_genus = np.NaN\n",
    "    draw_epithet = np.NaN\n",
    "    processed_df_dict = {}\n",
    "\n",
    "    for page_num in tqdm(index):\n",
    "        #print(page_num)\n",
    "        col_df_list = []\n",
    "        #process the pre-processed dfs\n",
    "        page_df = pre_df_dict[page_num]\n",
    "\n",
    "        #processing each column\n",
    "        for c in page_df['col_no'].unique():\n",
    "            col_df = page_df[page_df['col_no'] == c]\n",
    "            col_df, genus, epithet, draw_genus, draw_epithet = process_col(col_df, genus, epithet, draw_genus, draw_epithet)\n",
    "            col_df_list.append(col_df)\n",
    "\n",
    "        page_df = pd.concat(col_df_list, axis = 0)\n",
    "        processed_df_dict[page_num] = page_df\n",
    "    \n",
    "    return processed_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drawing ROI boxes in PDF\n",
    "def plot_ROI(processed_df_dict, index):\n",
    "    result_ims = []\n",
    "    for page_num in tqdm(index):\n",
    "        page_df = processed_df_dict[page_num]\n",
    "        \n",
    "        pix_map = doc.get_page_pixmap(page_num,matrix=mat)\n",
    "        image = Image.open(io.BytesIO(pix_map.tobytes()))\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        \n",
    "        plot_genus_blocks(col_df, draw)\n",
    "        plot_epithet_blocks(col_df, draw)\n",
    "        plot_author_blocks(col_df, draw)\n",
    "        plot_infra_blocks(col_df, draw)\n",
    "\n",
    "        result_ims.append(image)\n",
    "    return result_ims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving pdfs\n",
    "def save_outputs(processed_df_dict, index, output_name, make_csv = True, make_ROI_pdf = True, make_html = True, pruned = True):\n",
    "    df_list = [processed_df_dict[df_index] for df_index in processed_df_dict]\n",
    "    df = pd.concat(df_list, axis = 0)\n",
    "    print(\"merged dataframe\")\n",
    "\n",
    "    pruned_df = df[(~df['genus'].isnull())]\n",
    "    pruned_df = pruned_df[[\"page_num\", \"genus\", \"epithet\", \"infra\" ,\"author\", \"taxon rank\"]]\n",
    "    print(\"pruned dataframe\")\n",
    "\n",
    "    if make_csv:\n",
    "        df.to_csv('../output/index/CSV/'+ output_name +'.csv', index = False)\n",
    "        print(\"made .csv file\")\n",
    "        if pruned:\n",
    "            pruned_df.to_csv('../output/index/CSV/'+ output_name +'_pruned.csv', index = False)\n",
    "\n",
    "    if make_html: \n",
    "        df.to_html('../output/index/'+ output_name +'.html')\n",
    "        print(\"made .hrml file\")\n",
    "        if pruned: \n",
    "            pruned_df.to_html('../output/index/'+ output_name +'_pruned.html')\n",
    "\n",
    "    if make_ROI_pdf:\n",
    "        print(\"making .pdf file of ROIs\")\n",
    "        result_ims = plot_ROI(processed_df_dict, index)\n",
    "        result_ims[0].save('../output/index/PDF/'+ output_name +'_ROI.pdf',save_all=True, append_images=result_ims[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some global variables\n",
    "TARGET_DPI = 300\n",
    "mat = fitz.Matrix(TARGET_DPI/ 72, TARGET_DPI/ 72)\n",
    "\n",
    "def process_index_pages(pdf_dir, index, output_name, make_csv = True, make_ROI_pdf = True, make_html = True, pruned = True):\n",
    "    #importing all pages\n",
    "    doc = fitz.open(pdf_dir)\n",
    "    pages = [doc[i] for i in range(doc.pageCount)]\n",
    "    #print(index)\n",
    "    print(\"initiating dataframe for\", output_name)\n",
    "    pre_df_dict, pruned_dict = preprocessing_pages(pages, index)\n",
    "    print(\"processing dataframe for\", output_name)\n",
    "    processed_df_dict = processing_pages(pre_df_dict, index)\n",
    "    print(\"saving results for\", output_name)\n",
    "    save_outputs(processed_df_dict, index, output_name, make_csv = True, make_ROI_pdf = True, make_html = True, pruned = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initiating dataframe for vol3_index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:09<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing dataframe for vol3_index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:23<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving results for vol3_index\n",
      "merged dataframe\n",
      "pruned dataframe\n",
      "made .csv file\n",
      "made .hrml file\n",
      "making .pdf file of ROIs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:08<00:00,  3.27it/s]\n"
     ]
    }
   ],
   "source": [
    "vol3_dir = \"../input/NOUVELLE FLORE DU LIBAN ET DE LA SYRIE 3.pdf\"\n",
    "vol3_index = list(range(555, 583))\n",
    "vol3_output = 'vol3_index'\n",
    "process_index_pages(vol3_dir, vol3_index, vol3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initiating dataframe for vol2_index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:06<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing dataframe for vol2_index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:18<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving results for vol2_index\n",
      "merged dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'genus'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'genus'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fn/y98vt_mj7gl5c447m5vtrn0c0000gn/T/ipykernel_99953/3447350123.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvol2_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m703\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m725\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvol2_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'vol2_index'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprocess_index_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvol2_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvol2_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvol2_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/fn/y98vt_mj7gl5c447m5vtrn0c0000gn/T/ipykernel_99953/825376028.py\u001b[0m in \u001b[0;36mprocess_index_pages\u001b[0;34m(pdf_dir, index, output_name, make_csv, make_ROI_pdf, make_html, pruned)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprocessed_df_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessing_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_df_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saving results for\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0msave_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_df_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_ROI_pdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/fn/y98vt_mj7gl5c447m5vtrn0c0000gn/T/ipykernel_99953/2697925891.py\u001b[0m in \u001b[0;36msave_outputs\u001b[0;34m(processed_df_dict, index, output_name, make_csv, make_ROI_pdf, make_html, pruned)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"merged dataframe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpruned_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'genus'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpruned_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpruned_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"page_num\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"genus\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"epithet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"infra\"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\"author\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"taxon rank\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pruned dataframe\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'genus'"
     ]
    }
   ],
   "source": [
    "vol2_dir = \"../input/NOUVELLE FLORE DU LIBAN ET DE LA SYRIE 2.pdf\"\n",
    "vol2_index = list(range(703, 725))\n",
    "vol2_output = 'vol2_index'\n",
    "process_index_pages(vol2_dir, vol2_index, vol2_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initiating dataframe for vol1_index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:07<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing dataframe for vol1_index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:16<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving results for vol1_index\n",
      "merged dataframe\n",
      "pruned dataframe\n",
      "made .csv file\n",
      "made .hrml file\n",
      "making .pdf file of ROIs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/23 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "page not in document",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fn/y98vt_mj7gl5c447m5vtrn0c0000gn/T/ipykernel_99953/1560352949.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvol1_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m616\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m639\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvol1_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'vol1_index'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprocess_index_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvol1_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvol1_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvol1_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/fn/y98vt_mj7gl5c447m5vtrn0c0000gn/T/ipykernel_99953/825376028.py\u001b[0m in \u001b[0;36mprocess_index_pages\u001b[0;34m(pdf_dir, index, output_name, make_csv, make_ROI_pdf, make_html, pruned)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprocessed_df_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessing_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_df_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saving results for\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0msave_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_df_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_ROI_pdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/fn/y98vt_mj7gl5c447m5vtrn0c0000gn/T/ipykernel_99953/2697925891.py\u001b[0m in \u001b[0;36msave_outputs\u001b[0;34m(processed_df_dict, index, output_name, make_csv, make_ROI_pdf, make_html, pruned)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmake_ROI_pdf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"making .pdf file of ROIs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mresult_ims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_ROI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_df_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mresult_ims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../output/index/PDF/'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0moutput_name\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'_ROI.pdf'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresult_ims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/fn/y98vt_mj7gl5c447m5vtrn0c0000gn/T/ipykernel_99953/1968630523.py\u001b[0m in \u001b[0;36mplot_ROI\u001b[0;34m(processed_df_dict, index)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpage_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_df_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpage_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mpix_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_page_pixmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpix_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mdraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/fitz/utils.py\u001b[0m in \u001b[0;36mget_page_pixmap\u001b[0;34m(doc, pno, matrix, colorspace, clip, alpha, annots)\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0mannots\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0malso\u001b[0m \u001b[0mrender\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m     \"\"\"\n\u001b[0;32m--> 897\u001b[0;31m     return doc[pno].get_pixmap(\n\u001b[0m\u001b[1;32m    898\u001b[0m         \u001b[0mmatrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolorspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolorspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mannots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m     )\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/fitz/fitz.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m   5397\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Page\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5399\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"page not in document\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5400\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: page not in document"
     ]
    }
   ],
   "source": [
    "vol1_dir = \"../input/NOUVELLE FLORE DU LIBAN ET DE LA SYRIE 1.pdf\"\n",
    "vol1_index = range(616, 639)\n",
    "vol1_output = 'vol1_index'\n",
    "process_index_pages(vol1_dir, vol1_index, vol1_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac2eaa0ea0ebeafcc7822e65e46aa9d4f966f30b695406963e145ea4a91cd4fc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}